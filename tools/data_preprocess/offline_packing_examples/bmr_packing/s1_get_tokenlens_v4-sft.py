#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
# ä½¿ç”¨æ–¹å¼
python s1_get_tokenlens_v4-sft.py --config ./configs/s1_config_MR_sft_780k.yaml
"""

import os
import json
import orjson
import threading
import logging
import psutil
import tempfile
import queue
import yaml
import argparse
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from heapq import merge
from PIL import Image
from jinja2 import Template
from transformers import AutoProcessor
from transformers import BitsAndBytesConfig
from qwen_vl_utils import fetch_image
from queue import Empty
import multiprocessing
from multiprocessing import Pool, Manager, Value

# å£°æ˜å…¨å±€çš„è·¨è¿›ç¨‹è®¡æ•°å™¨ï¼ˆåœ¨ä¸»æ¨¡å—ä¸­å®šä¹‰ï¼Œè®©å­è¿›ç¨‹ç»§æ‰¿ï¼‰
global_total_counter = None

# âœ… è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description="Token Length Processor")
parser.add_argument("--config", type=str, default="config.yaml", help="Path to config.yaml")
parser.add_argument("--log-level", type=str, default=None,
                    choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                    help="Override log level from config")
args = parser.parse_args()

# âœ… åŠ è½½é…ç½®æ–‡ä»¶
CONFIG_PATH = Path(args.config)
if not CONFIG_PATH.exists():
    raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG_PATH}")
with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
    cfg = yaml.safe_load(f)

# âœ… ä»é…ç½®ä¸­è¯»å–å‚æ•°ï¼Œè¦†ç›–åŸæœ‰å¸¸é‡
MAX_TOKEN_LEN = cfg['sample']['max_len']
task_type = cfg['sample']['task_type']
DEL_ONE_TOKEN = cfg['sample']['del_one_token']

DEFAULT_DIRECTORY = Path(cfg['data']['directory'])
OUTPUT_FILE = Path(cfg['data']['output_base'])
TOKEN_INFO_FILE = Path(cfg['data']['output_token'])
CKPT_DIR = cfg['model']['checkpoint']
MIN_PIXELS = cfg['image']['min_pixels']
MAX_PIXELS = cfg['image']['max_pixels']
image_resolution = cfg['image']['baidu_resolution']
TIME_OUT = cfg['processing']['time_out']
# å½’å¹¶å‚æ•°ï¼ˆä»…ä¸¤çº§ï¼šstage0 â†’ stage1ï¼‰
STAGE1_CHUNK = cfg['processing']['stage1_merge_chunk']
chunk_size = cfg['processing']['chunk_size']
n_workers = cfg['processing']['n_workers']
MIN_WORKERS = cfg['processing']['min_workers']
MAX_WORKERS = cfg['processing']['max_workers']
use_shm = cfg['logging']['use_shm']
log_level = cfg['logging']['level']
log_file = cfg['logging']['file']
if args.log_level:
    log_level = args.log_level.upper()

# æ—¥å¿—é…ç½® - è¯¦ç»†è®°å½•æ•°æ®æµå‘å’Œåˆå¹¶è¿‡ç¨‹
file_handler = logging.FileHandler(
    log_file,
    delay=True,
    encoding='utf-8'
)
stream_handler = logging.StreamHandler()

logging.basicConfig(
    level=log_level,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[file_handler, stream_handler]
)
logger = logging.getLogger(__name__)

EXTENSIONS = (".json", ".jpg")


temp_dir = '/dev/shm' if use_shm else None  # None è¡¨ç¤ºä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸´æ—¶ç›®å½•

def count_lines(file_path):
    """ç»Ÿè®¡æ–‡ä»¶æœ‰æ•ˆè¡Œæ•°ï¼ˆéç©ºä¸”å«åˆ†éš”ç¬¦ï¼‰"""
    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
        return 0
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return sum(1 for line in f if line.strip() and ':' in line.strip())
    except Exception as e:
        logger.error(f"âŒ ç»Ÿè®¡æ–‡ä»¶ {file_path} è¡Œæ•°å¤±è´¥: {str(e)}")
        return 0

def find_paired_files(directory):
    directory = Path(directory)
    files = os.listdir(directory)
    json_set = {f[:-5] for f in files if f.lower().endswith('.json')}
    img_set  = {f[:-4] for f in files if f.lower().endswith(('.jpg', '.jpeg'))}
    paired = json_set & img_set
    logger.info(f"æ‰¾åˆ° {len(paired)} å¯¹åŒ¹é…æ–‡ä»¶")
    return paired

def find_valid_files(fname_json, rel_img_path):
    from s1_mr_sft_data_proc_indcoding import split_json_file
    valid_names = split_json_file(
                    fname_json, 
                    rel_img_path,
                    chunk_dim=2000,
                    m=8
    )
    return valid_names

def find_valid_json(directory):
    directory = Path(directory)
    files = os.listdir(directory)
    json_set = {f[:-5] for f in files if f.lower().endswith('.json')}
    logger.info(f"æ‰¾åˆ° {len(json_set)} ä¸ªjsonæ–‡ä»¶")
    return json_set    

def write_base_names_to_file(base_names, output_file):
    """å°†é…å¯¹æ–‡ä»¶åå†™å…¥æ–‡ä»¶"""
    try:
        content = "\n".join(sorted(base_names)) + "\n"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info(f"â„¹ï¸ å·²å°† {len(base_names)} ä¸ªé…å¯¹æ–‡ä»¶åå†™å…¥ {output_file}")
    except Exception as e:
        logger.error(f"âŒ å†™å…¥ {output_file} å¤±è´¥: {str(e)}")
        raise


def read_lines_in_chunks(file_path, chunk_size):
    """æŒ‰å—è¯»å–æ–‡ä»¶å†…å®¹"""
    file_path = Path(file_path)
    if not file_path.exists():
        raise FileNotFoundError(f"{file_path} ä¸å­˜åœ¨")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        while True:
            chunk = [line.strip() for _, line in zip(range(chunk_size), f) if line.strip()]
            if not chunk:
                break
            logger.info(f"â„¹ï¸ è¯»å–æ•°æ®å—ï¼ŒåŒ…å« {len(chunk)} ä¸ªæ ·æœ¬")
            yield chunk


# é¢„ç¼–è¯‘æ¨¡æ¿
"""
Todo:
    1) æ”¾åˆ° .yaml ä¸­
    2) åŠ å…¥é â€jinja2+processorâ€œ æ”¯æŒï¼ˆç”¨æˆ·è‡ªå®šä¹‰å¤„ç†å‡½æ•°ï¼‰ 
"""
if task_type=="pretrain":
    CAP_TEMPLATE = Template("<|vision_start|><|image_pad|><|vision_end|>{{ captions[0].content }}<|im_end|>")
elif task_type=="sft":
    chat_template  = """{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{{ message['content'] | replace('<image>', '<|vision_start|><|image_pad|><|vision_end|>') }}<|im_end|>\n{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"""
    CAP_TEMPLATE = Template(chat_template)
    pass

def process_sample(json_path, img_path, processor):
    """å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œè¿”å›(token_len, æ–‡ä»¶å)"""
    try:
        if not Path(json_path).exists():
            raise FileNotFoundError(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
        # if not Path(img_path).exists():
        #     raise FileNotFoundError(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {img_path}")

        # è¯»å–å¹¶æ¸²æŸ“JSONå†…å®¹
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        # with open(json_path, 'rb') as f:
        #     json_data = orjson.loads(f.read())
        if task_type=="pretrain":
            txt_input = CAP_TEMPLATE.render(captions=json_data['captions'])
        elif task_type=="sft":
            # txt_input = CAP_TEMPLATE.render(json_data)
            txt_input = CAP_TEMPLATE.render(json_data,tokenize=False, add_generation_prompt=False)
        if img_path=="_____.jpg":
            img_input = None
        else:
            def baidu_img_proc(image, image_resolution):
                image = Image.open(image)
                if max(image.width, image.height) > image_resolution:
                    resize_factor = image_resolution / max(image.width, image.height)
                    width, height = int(image.width * resize_factor), int(image.height * resize_factor)
                    image = image.resize((width, height), resample=Image.NEAREST)

                return image

            if image_resolution:
                img_path = baidu_img_proc(img_path, image_resolution)
                
            
            img_input = fetch_image({
                'type': 'image',
                'image': img_path,
                "min_pixels": MIN_PIXELS,
                "max_pixels": MAX_PIXELS,
            })
        # print(img_input)
        # è®¡ç®—tokené•¿åº¦
        base_name = Path(json_path).stem
        inputs = processor(
            text=[txt_input],
            images=img_input,
            videos=None,
            padding=True,
            return_tensors="pt",
        )
        # print(inputs["input_ids"])
        # print(inputs["input_ids"].shape)
        return (inputs["input_ids"].shape[1], base_name)

    except Exception as e:
        return (None, f"âŒ å¤„ç†å¤±è´¥ [{Path(json_path).stem}]: {str(e)}")


def get_adaptive_workers(min_workers=20, max_workers=96):
    """æ ¹æ®ç³»ç»Ÿè´Ÿè½½è°ƒæ•´çº¿ç¨‹æ•°"""
    try:
        cpu_usage = psutil.cpu_percent(interval=0.5)
        mem_usage = psutil.virtual_memory().percent
        if cpu_usage > 80 or mem_usage > 85:
            adjusted = max(min_workers, max_workers // 2)
            logger.info(f"ç³»ç»Ÿè´Ÿè½½è¿‡é«˜ï¼Œçº¿ç¨‹æ•°è°ƒæ•´ä¸º {adjusted} (CPU: {cpu_usage}%, å†…å­˜: {mem_usage}%)")
            return adjusted
        return max_workers
    except Exception as e:
        logger.warning(f"è·å–ç³»ç»Ÿè´Ÿè½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çº¿ç¨‹æ•° {max_workers}: {str(e)}")
        return max_workers

gt_maxlen=0
def merge_files_by_token(input_files, output_file, max_token=MAX_TOKEN_LEN):
    """åˆå¹¶å¤šä¸ªå·²æ’åºæ–‡ä»¶ï¼ŒæŒ‰token_lenå‡åºï¼Œè¿‡æ»¤æ‰ > max_token çš„æ•°æ®ï¼Œè¿”å›(è¾“å‡ºè·¯å¾„, æ•°æ®æ¡æ•°)"""
    if not input_files:
        logger.warning("âš ï¸ æ²¡æœ‰æ–‡ä»¶å¯åˆå¹¶")
        return (None, 0)

    # éªŒè¯è¾“å…¥æ–‡ä»¶å¹¶ç»Ÿè®¡æ€»æ•°æ®é‡
    valid_files = []
    total_lines = 0
    for f in input_files:
        line_count = count_lines(f)
        if line_count > 0:
            valid_files.append(f)
            total_lines += line_count
            logger.debug(f"â„¹ï¸ å¾…åˆå¹¶æ–‡ä»¶ {os.path.basename(f)} åŒ…å« {line_count} æ¡æ•°æ®")
        else:
            logger.warning(f"âš ï¸ æ–‡ä»¶ {os.path.basename(f)} ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡")

    if not valid_files:
        return (None, 0)

    # å®šä¹‰æ’åºé”®ï¼ˆæŒ‰token_lenæ•´æ•°æ’åºï¼‰
    def sort_key(line):
        _, token_str = line.strip().split(':', 1)
        return int(token_str)

    try:
        with open(output_file, 'w', encoding='utf-8') as out_f:
            # åˆ›å»ºæ‰€æœ‰æ–‡ä»¶çš„è¿­ä»£å™¨
            iterators = []
            file_handles = []
            for fpath in valid_files:
                try:
                    fh = open(fpath, 'r', encoding='utf-8')
                    file_handles.append(fh)
                    iterators.append(((sort_key(line), line) for line in fh))
                except Exception as e:
                    logger.error(f"âŒ æ‰“å¼€æ–‡ä»¶ {os.path.basename(fpath)} å¤±è´¥: {str(e)}")

            # # å½’å¹¶æ’åºå¹¶å†™å…¥
            # for _, line in merge(*iterators, key=lambda x: x[0]):
            #     out_f.write(line)
            # å½’å¹¶æ’åºå¹¶å†™å…¥ï¼Œè¿‡æ»¤æ‰ > max_token çš„è¡Œ(åç»­å¯æ·»åŠ å…¶ä»–æ¡ä»¶)
            filtered_max_len = 0
            for _, line in merge(*iterators, key=lambda x: x[0]):
                _, token_str = line.strip().split(':', 1)
                if int(token_str) <= max_token:   # â† åªä¿ç•™ â‰¤ 8192
                    out_f.write(line)
                else:
                    logger.warning(f"âš ï¸ tokené•¿åº¦ï¼š{token_str} > {max_token}: å‰”é™¤!!")
                    filtered_max_len+=1
                    gt_maxlen

            # å…³é—­æ‰€æœ‰æ–‡ä»¶å¥æŸ„
            for fh in file_handles:
                try:
                    fh.close()
                except Exception as e:
                    logger.warning(f"âš ï¸ å…³é—­æ–‡ä»¶ {fh.name} å¤±è´¥: {str(e)}")

        # éªŒè¯è¾“å‡ºæ–‡ä»¶æ•°æ®å®Œæ•´æ€§
        output_lines = count_lines(output_file)+filtered_max_len
        if output_lines != total_lines:   # è¿‡æ»¤æ‰ä¸æ»¡è¶³æ¡ä»¶çš„
            logger.error(f"âŒ åˆå¹¶æ•°æ®ä¸¢å¤±ï¼è¾“å…¥ {total_lines} æ¡ï¼Œè¾“å‡º {output_lines} æ¡ï¼Œå·²åˆ é™¤é”™è¯¯æ–‡ä»¶")
            if os.path.exists(output_file):
                os.remove(output_file)
            return (None, 0)
        else:
            logger.info(f"âœ… ğŸ“Š åˆå¹¶æˆåŠŸï¼Œè¾“å…¥ {total_lines} æ¡ï¼Œè¾“å‡º {output_lines-filtered_max_len} æ¡ï¼ˆtoken â‰¤ {max_token}ï¼‰çš„æ•°æ®")

        return (output_file, output_lines-filtered_max_len)
    except Exception as e:
        logger.error(f"âŒ åˆå¹¶æ–‡ä»¶å¤±è´¥: {str(e)}")
        if os.path.exists(output_file):
            try:
                os.remove(output_file)
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ é™¤å¤±è´¥æ–‡ä»¶ {output_file} å¤±è´¥: {str(e)}")
        return (None, 0)


def stage1_merger(input_queue, chunk_size, stage1_files, stop_event):
    """
    ä¿®å¤ç‰ˆstage1åˆå¹¶çº¿ç¨‹
    - ç¡®ä¿æ‰€æœ‰stage0æ–‡ä»¶è¢«åˆå¹¶ï¼ŒåŒ…æ‹¬æœ€åä¸è¶³10ä¸ªçš„æ–‡ä»¶
    - è§£å†³çº¿ç¨‹è¶…æ—¶å’Œæ•°æ®ä¸¢å¤±é—®é¢˜
    """
    buffer = []
    batch_counter = 0
    logger.info(f"ğŸ’¡ stage1åˆå¹¶çº¿ç¨‹å¯åŠ¨ï¼Œæ¯ {chunk_size} ä¸ªstage0æ–‡ä»¶åˆå¹¶ä¸€æ¬¡")

    try:
        # å¾ªç¯æ¡ä»¶ï¼šé˜Ÿåˆ—æœ‰æ–‡ä»¶ æˆ– ç¼“å†²åŒºæœ‰æ–‡ä»¶ æˆ– æœªæ”¶åˆ°åœæ­¢ä¿¡å·
        while (not input_queue.empty()) or buffer or (not stop_event.is_set()):
            # ä»é˜Ÿåˆ—å–æ–‡ä»¶ï¼ˆå¸¦è¶…æ—¶é˜²æ­¢æ°¸ä¹…é˜»å¡ï¼‰
            if not input_queue.empty():
                try:
                    file_path = input_queue.get(timeout=1)  # è¶…æ—¶1ç§’ï¼Œé¿å…æ°¸ä¹…é˜»å¡
                    buffer.append(file_path)
                    input_queue.task_done()
                    logger.debug(f"â„¹ï¸ stage1æ¥æ”¶æ–‡ä»¶ {os.path.basename(file_path)}ï¼Œå½“å‰ç¼“å†²åŒº: {len(buffer)}/{chunk_size}")

                    # è¾¾åˆ°åˆå¹¶æ•°é‡åˆ™æ‰§è¡Œåˆå¹¶
                    if len(buffer) >= chunk_size:
                        batch_counter += 1
                        merged_file = tempfile.NamedTemporaryFile(
                            mode='w', delete=False,
                            prefix=f"stage1_batch{batch_counter:03d}_",
                            encoding='utf-8',
                            dir=temp_dir
                        ).name
                        
                        # æ‰§è¡Œåˆå¹¶
                        merged_path, line_count = merge_files_by_token(buffer, merged_file)
                        if merged_path and line_count > 0:
                            stage1_files.append(merged_path)
                            logger.info(f"ğŸ“Š stage1æ‰¹æ¬¡ {batch_counter} å®Œæˆ: {os.path.basename(merged_path)}ï¼ŒåŒ…å« {line_count} æ¡æ•°æ®ï¼ˆåˆå¹¶äº† {len(buffer)} ä¸ªæ–‡ä»¶ï¼‰")
                        else:
                            logger.warning(f"âš ï¸ stage1æ‰¹æ¬¡ {batch_counter} åˆå¹¶å¤±è´¥ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")

                        # æ¸…ç©ºç¼“å†²åŒº
                        buffer = []
                except Empty:
                    continue  # é˜Ÿåˆ—ä¸ºç©ºæ—¶ç»§ç»­å¾ªç¯
                except Exception as e:
                    logger.error(f"âŒ stage1å¤„ç†æ–‡ä»¶æ—¶é”™è¯¯: {str(e)}", exc_info=True)
            else:
                # é˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åˆå¹¶å‰©ä½™æ–‡ä»¶
                if buffer and stop_event.is_set():
                    # æ”¶åˆ°åœæ­¢ä¿¡å·ä¸”ç¼“å†²åŒºæœ‰æ–‡ä»¶ï¼Œå¼ºåˆ¶åˆå¹¶
                    batch_counter += 1
                    merged_file = tempfile.NamedTemporaryFile(
                        mode='w', delete=False,
                        prefix=f"stage1_remaining_batch{batch_counter:03d}_",
                        encoding='utf-8',
                        dir=temp_dir
                    ).name
                    
                    merged_path, line_count = merge_files_by_token(buffer, merged_file)
                    if merged_path and line_count > 0:
                        stage1_files.append(merged_path)
                        logger.info(f"ğŸ“Š stage1å‰©ä½™æ–‡ä»¶åˆå¹¶å®Œæˆ: {os.path.basename(merged_path)}ï¼ŒåŒ…å« {line_count} æ¡æ•°æ®ï¼ˆåˆå¹¶äº† {len(buffer)} ä¸ªæ–‡ä»¶ï¼‰")
                    else:
                        logger.warning(f"âŒ stage1å‰©ä½™æ–‡ä»¶åˆå¹¶å¤±è´¥ï¼Œæ•°æ®å¯èƒ½ä¸¢å¤±")
                    buffer = []
                else:
                    # çŸ­æš‚ä¼‘çœ ï¼Œå‡å°‘CPUå ç”¨
                    threading.Event().wait(0.5)

        # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿ç¼“å†²åŒºä¸ºç©ºï¼ˆé˜²æ­¢é—æ¼ï¼‰
        if buffer:
            logger.error(f"âŒ stage1çº¿ç¨‹é€€å‡ºæ—¶ç¼“å†²åŒºä»æœ‰ {len(buffer)} ä¸ªæ–‡ä»¶æœªå¤„ç†ï¼æ•°æ®ä¸¢å¤±")

    except Exception as e:
        logger.error(f"âŒ stage1çº¿ç¨‹å¼‚å¸¸é€€å‡º: {str(e)}", exc_info=True)
    finally:
        logger.info(f"ğŸ“Š stage1çº¿ç¨‹é€€å‡ºï¼Œå…±ç”Ÿæˆ {len(stage1_files)} ä¸ªæ–‡ä»¶")

# æ–°å¢ï¼šæ¯ä¸ªè¿›ç¨‹çš„å¤„ç†å‡½æ•°ï¼ˆè´Ÿè´£å¤„ç†ä¸€ä¸ªå¤§chunkï¼‰
def process_chunk(args):
    """
    å•ä¸ªè¿›ç¨‹çš„å¤„ç†é€»è¾‘ï¼šå¤„ç†ä¸€ä¸ªå¤§chunkï¼Œå†…éƒ¨ç”¨å¤šçº¿ç¨‹å¹¶è¡Œ
    
    Args:
        args: åŒ…å«chunkæ•°æ®ã€å¤„ç†å™¨é…ç½®ã€é˜Ÿåˆ—ç­‰å‚æ•°çš„å…ƒç»„
    """
    # ä»å…¨å±€å˜é‡è·å–è®¡æ•°å™¨ï¼Œè€Œéå‚æ•°
    global global_total_counter
    
    chunk_idx, chunk, ckpt_dir, min_pixels, max_pixels, stage0_queue = args
    processor = None
    processed_count = 0  # è®°å½•å½“å‰è¿›ç¨‹å¤„ç†çš„æœ‰æ•ˆæ ·æœ¬æ•°
    
    
    try:
        # æ¯ä¸ªè¿›ç¨‹å•ç‹¬åˆå§‹åŒ–å¤„ç†å™¨ï¼ˆè¿›ç¨‹é—´ä¸èƒ½å…±äº«processorå®ä¾‹ï¼‰
        # quant_config = BitsAndBytesConfig(load_in_4bit=True)
        processor = AutoProcessor.from_pretrained(
            ckpt_dir,
            min_pixels=min_pixels,
            max_pixels=max_pixels,
            trust_remote_code=True,
            use_fast=False
        )
        # ç”Ÿæˆå½“å‰chunkçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        full_paths = []
        for fn in chunk:
            cur_json = str(DEFAULT_DIRECTORY / f"{fn}.json")
            # logger.info(f"ğŸ‘‰ è¿›ç¨‹ {multiprocessing.current_process().name} jsonæ–‡ä»¶ï¼š{cur_json}.....{type(cur_json)}")
            if f"{fn}.json".startswith("__img--output_"):
                cur_img = "_____.jpg"
                # cur_img = str(DEFAULT_DIRECTORY / f"{cur_img}")
            else:     
                with open(cur_json, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    cur_img = data['images'][0]
                    cur_img = str(DEFAULT_DIRECTORY / f"{cur_img}")
            full_paths.append(cur_json)
            full_paths.append(cur_img)
            # print(f"--------------cur_json:{cur_json}, cur_img:{cur_img}-------------------")
            

        n_samples = len(chunk)
        logger.info(f"ğŸ‘‰ è¿›ç¨‹ {multiprocessing.current_process().name} å¼€å§‹å¤„ç†å— {chunk_idx}ï¼ŒåŒ…å« {n_samples} ä¸ªæ ·æœ¬")
        
        # è¿›ç¨‹å†…åˆ›å»ºçº¿ç¨‹æ± ï¼ˆå¤ç”¨çº¿ç¨‹ï¼‰
        n_workers = get_adaptive_workers(min_workers=MIN_WORKERS, max_workers=MAX_WORKERS)  # å•ä¸ªè¿›ç¨‹çš„çº¿ç¨‹æ•°å¯é€‚å½“å‡å°‘
        chunk_results = []
        with ThreadPoolExecutor(
            max_workers=n_workers,
            thread_name_prefix=f"proc-{multiprocessing.current_process().pid}-thread"
        ) as executor:
            tasks = [
                executor.submit(
                    process_sample,
                    full_paths[idx*2],
                    full_paths[idx*2+1],
                    processor
                ) for idx in range(n_samples)
            ]
            
            # æ”¶é›†çº¿ç¨‹ä»»åŠ¡ç»“æœ
            for future in as_completed(tasks):
                try:
                    token_len, name = future.result()
                    if DEL_ONE_TOKEN:
                        token_len += 1
                    if token_len is not None:
                        chunk_results.append((token_len, name))
                        processed_count += 1  # ç»Ÿè®¡æœ‰æ•ˆæ ·æœ¬
                    else:
                        logger.warning(name)
                except Exception as e:
                    logger.error(f"âŒ è¿›ç¨‹å†…ä»»åŠ¡é”™è¯¯: {str(e)}")
        
        # å†™å…¥stage0æ–‡ä»¶å¹¶æ”¾å…¥è·¨è¿›ç¨‹é˜Ÿåˆ—
        if chunk_results:
            chunk_results_sorted = sorted(chunk_results, key=lambda x: x[0])
            with tempfile.NamedTemporaryFile(
                mode='w+', delete=False,
                prefix=f"stage0_chunk{chunk_idx:03d}_",
                encoding='utf-8',
                dir=temp_dir  
            ) as f:
                stage0_file = f.name
                for token_len, name in chunk_results_sorted:
                    f.write(f"{name}:{token_len}\n")
            
            line_count = count_lines(stage0_file)
            stage0_queue.put(stage0_file)  # æ”¾å…¥è·¨è¿›ç¨‹é˜Ÿåˆ—
            # logger.info(f"è¿›ç¨‹ {multiprocessing.current_process().name} å®Œæˆå— {chunk_idx}ï¼Œç”Ÿæˆ {line_count} æ¡æ•°æ®")
            # logger.info(f"ï¿½ï¿½ è¿›ç¨‹ {multiprocessing.current_process().name} å®Œæˆå— {chunk_idx}ï¼Œæœ‰æ•ˆæ ·æœ¬ {processed_count}/{n_samples}")
            proc_status = "ğŸŸ¢" if processed_count==n_samples else "ğŸŸ¡"
            logger.info(f"{proc_status} è¿›ç¨‹ {multiprocessing.current_process().name} å®Œæˆå— {chunk_idx}ï¼Œæœ‰æ•ˆæ ·æœ¬ {processed_count}/{n_samples}")
            
            # ã€å…³é”®ã€‘è·¨è¿›ç¨‹ç´¯åŠ æ€»æ•°æ®é‡ï¼ˆä½¿ç”¨ValueåŸå­æ“ä½œï¼‰
            with global_total_counter.get_lock():
                global_total_counter.value += processed_count
                
            return stage0_file  # è¿”å›ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºåç»­æ¸…ç†
        
    except Exception as e:
        logger.error(f"âŒ è¿›ç¨‹ {multiprocessing.current_process().name} å¤„ç†å¤±è´¥: {str(e)}")
    finally:
        if processor:
            del processor
    return None


###
def main():
    global global_total_counter  # å¼•ç”¨å…¨å±€å˜é‡
    processor = None   # æ¨¡å‹å¤„ç†å™¨å®ä¾‹
    stage0_files = []  # è®°å½•æ‰€æœ‰stage0æ–‡ä»¶ï¼ˆç”¨äºéªŒè¯å’Œæ¸…ç†ï¼‰
    stage1_files = []  # è®°å½•æ‰€æœ‰stage1æ–‡ä»¶ï¼ˆç”¨äºæœ€ç»ˆåˆå¹¶ï¼‰

    try:

        logger.info(f"ğŸ’¡ --------------å¼€å§‹æ•°æ®å¤„ç†æµç¨‹--------------")
        
        # 1. æŸ¥æ‰¾é…å¯¹æ–‡ä»¶å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆjsonå’Œjpgæ–‡ä»¶åç›¸åŒçš„æ ·æœ¬ï¼‰
        # base_names = find_paired_files(DEFAULT_DIRECTORY)    # DEFAULT_DIRECTORY æ˜¯åŸå§‹æ•°æ®å­˜æ”¾ä½ç½®ï¼ˆjpg å’Œ jsonï¼‰
        base_names = find_valid_json(DEFAULT_DIRECTORY)
        total_original = len(base_names)  # åŸå§‹æ ·æœ¬æ€»æ•°
        logger.info(f"ğŸ‘‰ æ‰¾åˆ° {total_original} å¯¹åŸå§‹æ ·æœ¬æ–‡ä»¶")
        if total_original == 0:
            logger.warning("âš ï¸ æ— åŸå§‹æ ·æœ¬ï¼Œé€€å‡ºç¨‹åº")
            return
        # å°†é…å¯¹æ–‡ä»¶åå†™å…¥æ–‡ä»¶ï¼Œç”¨äºåç»­åˆ†å—è¯»å–
        write_base_names_to_file(base_names, OUTPUT_FILE)
        
        # 2. åˆå§‹åŒ–è·¨è¿›ç¨‹é˜Ÿåˆ—ï¼ˆç”¨äºä¼ é€’stage0æ–‡ä»¶è·¯å¾„ç»™åˆå¹¶çº¿ç¨‹ï¼‰
        manager = Manager()  # è¿›ç¨‹é—´å…±äº«é˜Ÿåˆ—éœ€è¦ç”¨Manager
        stage0_queue = manager.Queue()
        stop_event = manager.Event()  # è·¨è¿›ç¨‹åœæ­¢ä¿¡å·

        # è·¨è¿›ç¨‹è®¡æ•°å™¨ï¼Œç”¨äºç»Ÿè®¡æ€»å¤„ç†æ ·æœ¬æ•°ï¼ˆåˆå§‹å€¼0ï¼‰
        global_total_counter = Value('i', 0)  # 'i'è¡¨ç¤ºæ•´æ•°ç±»å‹

        # 3 å¯åŠ¨stage1åˆå¹¶çº¿ç¨‹ï¼ˆå®ˆæŠ¤çº¿ç¨‹ï¼‰
        stage1_thread = threading.Thread(
            target=stage1_merger,
            args=(stage0_queue, STAGE1_CHUNK, stage1_files, stop_event),
            daemon=True
        )
        stage1_thread.start()
        logger.info("ğŸ’¡ stage1åˆå¹¶çº¿ç¨‹å·²å¯åŠ¨")

        # 4. å¤„ç†æ•°æ®å¹¶ç”Ÿæˆstage0æ–‡ä»¶ï¼ˆæ¯å—æ•°æ®å•ç‹¬å¤„ç†å¹¶æ’åºï¼‰
        # n_workers = 96 #get_adaptive_workers()

        # 4.1 è¯»å–æ‰€æœ‰æ•°æ®å—ï¼ˆå‡†å¤‡åˆ†ç»™å¤šä¸ªè¿›ç¨‹)
        # chunk_size = chunk_size  # æ¯ä¸ªè¿›ç¨‹å¤„ç†çš„å¤§chunkå°ºå¯¸ï¼ˆæ ¹æ®å†…å­˜è°ƒæ•´ï¼‰
        all_chunks = list(read_lines_in_chunks(OUTPUT_FILE, chunk_size))
        total_chunks = len(all_chunks)
        n_processes = min(multiprocessing.cpu_count(), total_chunks)
        logger.info(f"ğŸ‘‰ åˆ’åˆ†ä¸º {total_chunks} ä¸ªå—ï¼Œå¯åŠ¨ {n_processes} ä¸ªè¿›ç¨‹å¤„ç†")

        # 4.2 å‡†å¤‡è¿›ç¨‹æ± å‚æ•°ï¼ˆåŒ…å«æ¨¡å‹é…ç½®ã€é˜Ÿåˆ—ç­‰ï¼‰
        process_args = [
            (
                idx + 1,  # chunkç´¢å¼•
                chunk,    # chunkæ•°æ®
                CKPT_DIR, # æ¨¡å‹è·¯å¾„
                MIN_PIXELS,
                MAX_PIXELS,
                stage0_queue,  # è·¨è¿›ç¨‹é˜Ÿåˆ—
            ) for idx, chunk in enumerate(all_chunks)
        ]
        
        # 4.3 å¯åŠ¨è¿›ç¨‹æ± ï¼ˆè¿›ç¨‹æ•°å»ºè®®è®¾ä¸ºCPUæ ¸å¿ƒæ•°çš„1~2å€ï¼‰
        with Pool(processes=n_processes) as process_pool:
            # å¹¶è¡Œå¤„ç†æ‰€æœ‰å¤§chunk
            # stage0_files = process_pool.map(process_chunk, process_args)
            result = process_pool.map_async(process_chunk, process_args)
            try:
                stage0_files = result.get(timeout=TIME_OUT)  # è¶…æ—¶è®¾ç½®
            except multiprocessing.TimeoutError:
                logger.error("âŒ éƒ¨åˆ†è¿›ç¨‹å¤„ç†è¶…æ—¶ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                process_pool.terminate()
        
        # è¿‡æ»¤ç©ºç»“æœ
        stage0_files = [f for f in stage0_files if f is not None]
        logger.info(f"âœ… æ‰€æœ‰è¿›ç¨‹å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(stage0_files)} ä¸ªstage0æ–‡ä»¶")  
        # ç»Ÿè®¡æ•°æ®
        total_processed = global_total_counter.value  # ç›´æ¥ä»å…¨å±€å˜é‡è·å–  # è·å–æ€»å¤„ç†æ ·æœ¬æ•°
        logger.info(f"ğŸ‘‰ åŸå§‹æ ·æœ¬æ•°: {total_original}, æœ‰æ•ˆå¤„ç†æ ·æœ¬æ•°: {total_processed}")

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        if total_processed != total_original:
            logger.warning(f"âŒ æ•°æ®ä¸å®Œæ•´ï¼åŸå§‹ {total_original} ä¸ªï¼Œæœ‰æ•ˆå¤„ç† {total_processed} ä¸ªï¼Œå·®å¼‚ {total_original - total_processed} ä¸ª")
        else:
            logger.info("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡ï¼Œæ‰€æœ‰æ ·æœ¬å‡è¢«æœ‰æ•ˆå¤„ç†")

        # 5. ç­‰å¾…å¤„ç†å®Œæˆï¼ˆç¡®ä¿æ‰€æœ‰æ–‡ä»¶è¢«åˆå¹¶ï¼‰
        # ç­‰å¾…stage0é˜Ÿåˆ—æ‰€æœ‰æ–‡ä»¶è¢«å¤„ç†
        logger.info("ğŸ”„ ç­‰å¾…stage0é˜Ÿåˆ—å¤„ç†å®Œæˆ...")
        stage0_queue.join()  # é˜»å¡ç›´åˆ°æ‰€æœ‰stage0æ–‡ä»¶è¢«æ¶ˆè´¹
        logger.info("ğŸ’¡ stage0é˜Ÿåˆ—æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæ¯•")

        # å‘é€åœæ­¢ä¿¡å·ç»™stage1çº¿ç¨‹ï¼Œå¼ºåˆ¶å¤„ç†å‰©ä½™æ–‡ä»¶
        logger.info("ğŸ’¡ é€šçŸ¥stage1çº¿ç¨‹åœæ­¢å¹¶å¤„ç†å‰©ä½™æ–‡ä»¶...")
        stop_event.set()

        # å»¶é•¿è¶…æ—¶æ—¶é—´è‡³60ç§’ï¼Œç¡®ä¿å¤§æ–‡ä»¶åˆå¹¶å®Œæˆ
        timeout_counter = 0
        while stage1_thread.is_alive() and timeout_counter < 60:
            logger.debug(f"ğŸ”„ ç­‰å¾…stage1çº¿ç¨‹å®Œæˆï¼ˆ{timeout_counter}/60ç§’ï¼‰")
            threading.Event().wait(1)  # ç­‰å¾…1ç§’åé‡è¯•
            timeout_counter += 1
        
        if stage1_thread.is_alive():
            logger.warning("âš ï¸ stage1çº¿ç¨‹è¶…æ—¶æœªé€€å‡ºï¼Œå¯èƒ½å­˜åœ¨å¼‚å¸¸ï¼ˆä½†å·²å°è¯•å¼ºåˆ¶åˆå¹¶å‰©ä½™æ–‡ä»¶ï¼‰")
        else:
            logger.info("ğŸ’¡ stage1çº¿ç¨‹å·²æ­£å¸¸é€€å‡º")

        # éªŒè¯stage1æ–‡ä»¶æ•°é‡æ˜¯å¦åŒ¹é…ï¼ˆæ¯10ä¸ªstage0åˆå¹¶1ä¸ªï¼Œä¸è¶³10ä¸ªä¹Ÿç®—1ä¸ªï¼‰
        expected_stage1_count = (len(stage0_files) + STAGE1_CHUNK - 1) // STAGE1_CHUNK
        if len(stage1_files) != expected_stage1_count:
            logger.warning(f"âš ï¸ â„¹ï¸  stage1æ–‡ä»¶æ•°é‡å¼‚å¸¸ï¼é¢„æœŸ {expected_stage1_count} ä¸ªï¼Œå®é™… {len(stage1_files)} ä¸ª")
        else:
            logger.info(f"âœ… stage1æ–‡ä»¶æ•°é‡éªŒè¯é€šè¿‡: {len(stage1_files)} ä¸ª")

        # 6. æœ€ç»ˆåˆå¹¶æ‰€æœ‰stage1æ–‡ä»¶åˆ°token_info_1.txt
        if not stage1_files:
            logger.warning("âš ï¸ æ²¡æœ‰ç”Ÿæˆstage1æ–‡ä»¶ï¼Œæ£€æŸ¥ä¸­é—´å¤„ç†æ˜¯å¦å‡ºé”™")
            return

        # ç»Ÿè®¡stage1æ–‡ä»¶æ€»æ•°æ®é‡
        stage1_total = sum(count_lines(f) for f in stage1_files)
        logger.info(f"â„¹ï¸ å¼€å§‹æœ€ç»ˆåˆå¹¶: {len(stage1_files)} ä¸ªstage1æ–‡ä»¶ï¼Œæ€»æ•°æ®é‡: {stage1_total} æ¡")

        # åˆå¹¶åˆ°æœ€ç»ˆæ–‡ä»¶
        final_path, final_lines = merge_files_by_token(stage1_files, TOKEN_INFO_FILE)

        if final_path and final_lines > 0:
            logger.info(f"âœ… æœ€ç»ˆç»“æœæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {TOKEN_INFO_FILE}ï¼ŒåŒ…å« {final_lines} æ¡æ•°æ®")
            # éªŒè¯æ€»æ•°æ®é‡
            if final_lines != total_processed:
                logger.error(f"âŒ æ•°æ®é‡ä¸ä¸€è‡´ï¼å¤„ç†æ€»æ•°æ® {total_processed} æ¡ï¼Œæœ€ç»ˆæ–‡ä»¶ {final_lines} æ¡")
            else:
                logger.info("âœ…ğŸ’¡ æ•°æ®é‡éªŒè¯é€šè¿‡ï¼Œæ‰€æœ‰æ•°æ®å·²æ­£ç¡®å†™å…¥æœ€ç»ˆæ–‡ä»¶")
        else:
            logger.error("âŒ æœ€ç»ˆæ–‡ä»¶åˆå¹¶å¤±è´¥")

        # æœ€ç»ˆåˆå¹¶åå†æ¬¡éªŒè¯
        if os.path.exists(TOKEN_INFO_FILE):
            final_count = count_lines(TOKEN_INFO_FILE)
            logger.info(f"â„¹ï¸ æœ€ç»ˆç»“æœæ–‡ä»¶åŒ…å« {final_count} æ¡æ•°æ®")
            if final_count != total_processed:
                logger.error(f"âŒ æœ€ç»ˆæ–‡ä»¶æ•°æ®ä¸å®Œæ•´ï¼å¤„ç† {total_processed} æ¡ï¼Œæœ€ç»ˆæ–‡ä»¶ {final_count} æ¡")
            else:
                logger.info("âœ… æœ€ç»ˆæ–‡ä»¶æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")

    except Exception as e:
        logger.error(f"âŒ ä¸»æµç¨‹é”™è¯¯: {str(e)}", exc_info=True)
    finally:
        # æ¸…ç†èµ„æº
        if processor:
            del processor

        # ç¡®ä¿åœæ­¢ä¿¡å·è¢«è§¦å‘
        stop_event.set()

        if stage1_thread and stage1_thread.is_alive():
            stage1_thread.join(timeout=2)        
        
        # ç­‰å¾…æœ€ç»ˆæ–‡ä»¶å†™å…¥å®Œæˆ
        threading.Event().wait(2)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆä¿ç•™æœ€ç»ˆæ–‡ä»¶ï¼‰
        all_temp_files = stage0_files + stage1_files
        for fpath in all_temp_files:
            if fpath != str(TOKEN_INFO_FILE) and os.path.exists(fpath):
                try:
                    os.remove(fpath)
                    logger.debug(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {os.path.basename(fpath)}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {os.path.basename(fpath)}: {str(e)}")

        logger.info("ç¨‹åºæ‰§è¡Œå®Œæ¯•")


if __name__ == "__main__":
    main()

