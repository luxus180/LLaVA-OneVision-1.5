import os
import sys
import time
import logging
import numpy as np
from pathlib import Path
from itertools import islice
from tqdm import tqdm
from collections import defaultdict
from typing import List, Tuple, Dict, Optional, Union, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import queue
import bisect

class HashBucketProcessor:
    """å“ˆå¸Œæ¡¶å¤„ç†å™¨ï¼Œç”¨äºå¤„ç†å¤§å‹æ•°æ®æ–‡ä»¶å¹¶è¿›è¡Œé«˜æ•ˆè£…ç®±"""
    
    DTYPE_SAMPLE_INFO = np.dtype([
        ("w", np.uint16),       # ç”¨äºå­˜å‚¨ ViT éƒ¨åˆ†çš„æƒé‡ï¼ˆå¯ä»¥æ˜¯ ViT éƒ¨åˆ†çš„åƒç´ æ•°æˆ–è€… ViT éƒ¨åˆ†çš„å¤„ç†èƒ½åŠ›ï¼‰
        ("l", np.uint16),       # ç”¨äºå­˜å‚¨ llm éƒ¨åˆ†çš„æ›²ç§ï¼ˆ LLM è¾“å…¥éƒ¨åˆ†çš„ tokens æ•°äº©ï¼‰
        ("name", "U64")        # sampleâ€˜s name
    ])

    def __init__(self, file_path: Union[str, Path], logger: Optional[logging.Logger] = None):
        self.file_path = Path(file_path)
        if not self.file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
        self.hash_buckets = defaultdict(lambda: np.array([], dtype=self.DTYPE_SAMPLE_INFO))
        self.total_lines = 0
        self.hb2_keys = []   # å¯ä»¥é™¤ä»¥å“ªäº› 2 çš„å¹‚æ¬¡
        self._logger = logger or self._setup_default_logger()

    @staticmethod
    def _setup_default_logger() -> logging.Logger:
        """è®¾ç½®é»˜è®¤æ—¥å¿—å™¨"""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def estimate_memory_usage(self) -> int:
        """ä¼°ç®—å½“å‰å“ˆå¸Œæ¡¶å†…å­˜å ç”¨"""
        total_size = sys.getsizeof(self.hash_buckets)
        for key, arr in self.hash_buckets.items():
            total_size += sys.getsizeof(key) + arr.nbytes
        return total_size
    
    def _count_file_lines(self) -> int:
        """è®¡ç®—æ–‡ä»¶æ€»è¡Œæ•°ï¼Œä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹æ³•"""
        try:
            with self.file_path.open('rb') as f:
                return sum(1 for _ in f)
        except Exception as e:
            self._logger.warning(f"å¿«é€Ÿè®¡æ•°å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ–¹æ³•: {e}")
            with self.file_path.open('r', encoding='utf-8') as f:
                return sum(1 for _ in f)

    def _parse_line(self, line: str) -> Optional[Tuple[int, int, str]]:
        """è§£æå•è¡Œæ•°æ®ï¼Œè¿”å› (w, l, name) æˆ– None"""
        line = line.strip()
        if ':' not in line:
            return None
            
        try:
            name, key_str = line.split(':', 1)
            key = int(key_str)
            if 0 <= key <= 65535:
                return (0, key, name)
        except (ValueError, IndexError):
            pass
        return None
        
    def _update_buckets(self, parsed_data: List[Tuple[int, int, str]]) -> None:
        """æ›´æ–°å“ˆå¸Œæ¡¶"""
        data_array = np.array(parsed_data, dtype=self.DTYPE_SAMPLE_INFO)
        unique_l_values = np.unique(data_array['l'])

        for l_val in unique_l_values:
            mask = data_array['l'] == l_val
            chunk = data_array[mask]
            
            if l_val in self.hash_buckets:
                self.hash_buckets[l_val] = np.concatenate([self.hash_buckets[l_val], chunk])
            else:
                self.hash_buckets[l_val] = chunk
                
    def build_buckets(self, chunk_size: int = 100000) -> None:
        """æ„å»ºå“ˆå¸Œæ¡¶"""
        self.total_lines = self._count_file_lines()
        self._logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶ï¼Œæ€»è¡Œæ•°: {self.total_lines}")
        
        with self.file_path.open('r', encoding='utf-8') as file:
            with tqdm(total=self.total_lines, unit='è¡Œ', desc='æ„å»ºå“ˆå¸Œæ¡¶') as pbar:
                while True:
                    lines = list(islice(file, chunk_size))
                    if not lines:
                        break

                    pbar.update(len(lines))
                    
                    # å¹¶è¡Œè§£ææ•°æ®
                    parsed_data = []
                    for line in lines:
                        parsed = self._parse_line(line)
                        if parsed:
                            parsed_data.append(parsed)

                    if parsed_data:
                        self._update_buckets(parsed_data)                                

    @staticmethod
    def factors_of_two(a: int, C: int) -> List[Tuple[int, int]]:
        """è¿”å›æ‰€æœ‰æ»¡è¶³ b * 2^n = a ä¸” b > C çš„ (b, n) å¯¹"""
        if a < 0 or C < 0:
            raise ValueError("a å¿…é¡»ä¸ºæ­£æ•´æ•°ï¼ŒC å¿…é¡»ä¸ºéè´Ÿæ•´æ•°")
        res = []
        n = 0
        b = a
        while b > C:
            res.append((b, n))
            if b & 1:
                break
            b >>= 1
            n += 1
        return res

    def find_items(self, capacity: int) -> defaultdict[np.ndarray]:
        """ä»å“ˆå¸Œæ¡¶ä¸­æŸ¥æ‰¾ç¬¦åˆæ¡ä»¶çš„é¡¹ç›®"""

        if not self.hash_buckets:
            self._logger.warning("å“ˆå¸Œæ¡¶ä¸ºç©ºï¼Œè¯·å…ˆæ„å»ºå“ˆå¸Œæ¡¶")
            return
            
        for key, value in self.hash_buckets.items():
            if not isinstance(value, np.ndarray) or value.dtype != self.DTYPE_SAMPLE_INFO:
                raise TypeError(f"å“ˆå¸Œæ¡¶æ•°æ®æ ¼å¼é”™è¯¯ï¼Œkey={key}")
            break
        self.hb2_keys=[]
        min_l_value = min(self.hash_buckets.keys())
        valid_b_values = [b for b, _ in self.factors_of_two(capacity, min_l_value - 1)]

        for b in valid_b_values:
            if b in self.hash_buckets:
                self.hb2_keys.append(b)

        self._logger.info(f"æ‰¾åˆ° {len(self.hb2_keys)} ä¸ªæœ‰æ•ˆçš„æ¡¶é”®")

    def delete_by_index(self, result: defaultdict[np.ndarray], key: int, index: int) -> None:
        """æŒ‰ç´¢å¼•åˆ é™¤å…ƒç´ """
        if key in result and 0 <= index < len(result[key]):
            result[key] = np.delete(result[key], index)

    def get_statistics(self) -> Dict[str, Union[int, float]]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_items = sum(len(arr) for arr in self.hash_buckets.values())
        memory_gb = self.estimate_memory_usage() / (1024**3)
        
        return {
            "bucket_count": len(self.hash_buckets),
            "total_items": total_items,
            "memory_usage_gb": memory_gb,
            "hb2_keys_count": [len(self.hb2_keys),self.hb2_keys],
            "file_lines": self.total_lines
        }

    def __len__(self) -> int:
        """è¿”å›æ€»æ•°æ®é¡¹æ•°"""
        return sum(len(arr) for arr in self.hash_buckets.values())

    def __repr__(self) -> str:
        return f"HashBucketProcessor(buckets={len(self.hash_buckets)}, items={len(self)})"    
        
    def summary(self) -> None:
        """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
        stats = self.get_statistics()
        print(f"=== å“ˆå¸Œæ¡¶å¤„ç†æ‘˜è¦ ===")
        print(f"å“ˆå¸Œæ¡¶æ•°é‡: {stats['bucket_count']}")
        print(f"æ€»æ•°æ®é¡¹: {stats['total_items']}")
        print(f"å†…å­˜å ç”¨: {stats['memory_usage_gb']:.2f} GB")
        print(f"æœ‰æ•ˆæ¡¶é”®: {stats['hb2_keys_count']}")
        print(f"å¤„ç†è¡Œæ•°: {stats['file_lines']}")    

    def _cleanup_empty_keys(self, verbose: bool = False) -> int:
        """
        æ¸…ç†å“ˆå¸Œæ¡¶ä¸­å…ƒç´ ä¸ªæ•°ä¸º0çš„key
        
        å‚æ•°:
            verbose: æ˜¯å¦æ‰“å°æ¸…ç†è¯¦æƒ…
        
        è¿”å›:
            int: åˆ é™¤çš„ç©ºkeyæ•°é‡
        """
        # 1. æ”¶é›†éœ€è¦åˆ é™¤çš„ç©ºkey
        empty_keys = []
        for key in list(self.hash_buckets.keys()):
            if len(self.hash_buckets[key]) == 0:
                empty_keys.append(key)
        
        # 2. åˆ é™¤ç©ºkey
        for key in empty_keys:
            del self.hash_buckets[key]
        
        # 3. è®°å½•æ—¥å¿—
        if verbose or empty_keys:
            self._logger.info(f"æ¸…ç†ç©ºkey: åˆ é™¤äº† {len(empty_keys)} ä¸ªç©ºkey")
            if verbose and empty_keys:
                self._logger.debug(f"åˆ é™¤çš„key: {sorted(empty_keys)}")
        
        return len(empty_keys)    

    def update_hash_buckets(self, remove_empty: bool = True, verbose: bool = False) -> dict:
        """
        æ›´æ–°å“ˆå¸Œæ¡¶ç»“æ„ï¼ŒåŒ…æ‹¬æ¸…ç†ç©ºkeyå’Œç»Ÿè®¡ä¿¡æ¯
        
        å‚æ•°:
            remove_empty: æ˜¯å¦åˆ é™¤ç©ºkey
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        è¿”å›:
            dict: æ›´æ–°åçš„ç»Ÿè®¡ä¿¡æ¯
        """
        # 1. åŸºç¡€ç»Ÿè®¡
        stats = {
            'before': {
                'total_keys': len(self.hash_buckets),
                'total_items': sum(len(arr) for arr in self.hash_buckets.values()),
                'empty_keys': sum(1 for arr in self.hash_buckets.values() if len(arr) == 0)
            }
        }
        
        # 2. å¯é€‰ï¼šåˆ é™¤ç©ºkey
        removed_keys = 0
        if remove_empty:
            removed_keys = self._cleanup_empty_keys(verbose)
        
        # 3. æ›´æ–°åç»Ÿè®¡
        stats['after'] = {
            'total_keys': len(self.hash_buckets),
            'total_items': sum(len(arr) for arr in self.hash_buckets.values()),
            'empty_keys': sum(1 for arr in self.hash_buckets.values() if len(arr) == 0)
        }
        
        # 4. è®¡ç®—å˜åŒ–
        stats['changes'] = {
            'keys_removed': removed_keys,
            'items_removed': stats['before']['total_items'] - stats['after']['total_items']
        }
        
        # 5. è®°å½•æ—¥å¿—
        if verbose or stats['changes']['keys_removed'] > 0:
            self._logger.info("å“ˆå¸Œæ¡¶æ›´æ–°å®Œæˆ:")
            self._logger.info(f"  ğŸ“Š Keyæ•°é‡: {stats['before']['total_keys']} â†’ {stats['after']['total_keys']}")
            self._logger.info(f"  ğŸ“¦ å…ƒç´ æ€»æ•°: {stats['before']['total_items']} â†’ {stats['after']['total_items']}")
            self._logger.info(f"  ğŸ—‘ï¸  åˆ é™¤ç©ºkey: {stats['changes']['keys_removed']}")
        
        return stats

    def get_hash_buckets_summary(self) -> dict:
        """
        è·å–å“ˆå¸Œæ¡¶çš„æ‘˜è¦ä¿¡æ¯
        
        è¿”å›:
            dict: åŒ…å«è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        # åŸºç¡€ç»Ÿè®¡
        total_keys = len(self.hash_buckets)
        total_items = sum(len(arr) for arr in self.hash_buckets.values())
        empty_keys = sum(1 for arr in self.hash_buckets.values() if len(arr) == 0)
        
        # æŒ‰å¤§å°åˆ†ç±»ç»Ÿè®¡
        size_distribution = {
            'large': 0,    # >= 8192
            'medium': 0,   # 2048-8192
            'small': 0     # < 2048
        }
        
        items_by_size = {
            'large': 0,
            'medium': 0,
            'small': 0
        }
        
        for key, arr in self.hash_buckets.items():
            count = len(arr)
            if key >= 8192:
                size_distribution['large'] += 1
                items_by_size['large'] += count
            elif key >= 2048:
                size_distribution['medium'] += 1
                items_by_size['medium'] += count
            else:
                size_distribution['small'] += 1
                items_by_size['small'] += count
        
        # è¿”å›å®Œæ•´æ‘˜è¦
        return {
            'basic': {
                'total_keys': total_keys,
                'total_items': total_items,
                'empty_keys': empty_keys,
                'non_empty_keys': total_keys - empty_keys
            },
            'size_distribution': size_distribution,
            'items_by_size': items_by_size,
            'memory_usage': self.estimate_memory_usage()
        }        
    
    def print_example(self, key: int) -> None:
        """æ‰“å°ç¤ºä¾‹æ•°æ®"""
        if key in self.hash_buckets:
            arr = self.hash_buckets[key]
            print(f"Key {key} çš„æ•°æ®æ•°é‡: {len(arr)}")
            print("å‰3æ¡æ•°æ®:")
            for item in arr[:3]:
                print(f"  w: {item['w']}, l: {item['l']}, name: {item['name']}")
        else:
            print(f"Key {key} ä¸å­˜åœ¨ã€‚")
  
    def pack_with_deletion(self, box_capacity: int = 16384) -> List[np.ndarray]:
        """æŒ‰å®¹é‡è£…ç®±ï¼Œä¼˜å…ˆå¤šæ ·æ€§ï¼Œè£…ç®±åç«‹å³ä»åŸæ¡¶ä¸­åˆ é™¤å·²ç”¨å…ƒç´ 
        ï¼ˆç”¨äºå•ç‹¬å¤„ç†  (box_capacity/key)==2^n çš„ key ï¼‰
        ä¸­é—´é‡åˆ°ä¸æ»¡ç®±æ—¶ï¼Œæ›´æ¢ 1æ¬¡ è£…ç®±ç­–ç•¥
        """
        from collections import deque
    
        boxes = []
    
        # ä¸ºæ¯ä¸ª key ç»´æŠ¤ä¸€ä¸ª dequeï¼Œæ–¹ä¾¿ pop ï¼ˆä»…è€ƒè™‘å­˜åœ¨å…ƒç´ çš„æ¡¶ï¼Œåé¢æ•°æ®é‡éå¸¸å¤§æ—¶å¯ä»¥è€ƒè™‘æ”¾å…¥ while å¾ªç¯ï¼‰
        key_queues = {k: deque(enumerate(self.hash_buckets[k])) 
                      for k in self.hb2_keys 
                      if k in self.hash_buckets and len(self.hash_buckets[k]) > 0}
    
        while any(key_queues.values()):
            current_box_items = []
            current_sum = 0
            used_indices = defaultdict(list)  # key -> list of indices to delete
    
            keys_to_try = deque(sorted(key_queues.keys()))
    
            while keys_to_try and current_sum < box_capacity:
                key = keys_to_try.popleft()
                queue = key_queues[key]
                if not queue:
                    continue
    
                idx, item = queue[0]
                l_val = key #item['l']
                if current_sum + l_val <= box_capacity:
                    queue.popleft()
                    current_box_items.append(item)
                    current_sum += l_val
                    used_indices[key].append(idx)
    
                    # å¦‚æœè¯¥ key è¿˜æœ‰å‰©ä½™ï¼Œæ”¾å›é˜Ÿåˆ—å°¾éƒ¨
                    if queue:
                        keys_to_try.append(key)
    
            if current_box_items and current_sum==box_capacity:
                # æ»¡ç®±ï¼šè¾“å‡ºå¹¶åˆ é™¤
                boxes.append(np.array(current_box_items, dtype=self.DTYPE_SAMPLE_INFO))
    
                # ä» self.hash_buckets ä¸­åˆ é™¤å·²ç”¨å…ƒç´ 
                for key, indices in used_indices.items():
                    indices = sorted(indices, reverse=True)
                    for idx in indices:
                        self.hash_buckets[key] = np.delete(self.hash_buckets[key], idx)
    
                    # æ›´æ–° key_queues ä¸­çš„ dequeï¼ˆæœªåˆ é™¤çš„åˆæ›´æ–°åˆ° key_queï¼‰
                    # æ›´æ–° key_queues ä¸­çš„ dequeï¼ˆç”¨æ‰çš„å…ƒç´ ï¼‰ï¼Œè¿™æ ·åšåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¯ä»¥é¿å…å®Œå…¨é™·å…¥æ­»å¾ªç¯ï¼ˆé™¤éæ‰€æœ‰é˜Ÿåˆ—å‰©ä½™å…ƒç´ æ•°ç›®å®Œå…¨ç›¸åŒï¼‰
                    key_queues[key] = deque(enumerate(self.hash_buckets[key]))
            else:
                # åŠ ä¸€ä¸ªåˆ¤æ–­ï¼Œå¦‚æœå„ä¸ªé˜Ÿåˆ—å…ƒç´ æ•°å®Œå…¨ç›¸åŒï¼Œåˆ™æ”¹å˜ä¸€æ¬¡ packing ç­–ç•¥(è¿™ç§æƒ…å†µè·³ä¸å‡ºå¾ªç¯â™»ï¸) ï¼Œå†å›åˆ°åŸå§‹æ–¹æ³•
                self._logger.info(f"å½“å‰ç®±å­æ²¡æœ‰æ»¡: {current_sum}")
                self._logger.info(f"å½“å‰ç®±å­å…ƒç´ : {current_box_items}")
                
                left_elems = [len(self.hash_buckets[k]) for k in self.hb2_keys if k in self.hash_buckets and len(self.hash_buckets[k])>0]
                # æ‹¼åŒ…å‰©ä½™çš„ key
                left_keys = [k for k in self.hb2_keys if k in self.hash_buckets and len(self.hash_buckets[k])>0]
                print(f"å‰©ä½™çš„keyåŠå…¶å…ƒç´ æ•°é‡ï¼š(keys, nums):({left_keys},{left_elems})")
                if len(set(left_elems)) == 1:
                    self._logger.info(f"æ”¹å˜æ‹¼åŒ…ç­–ç•¥ï¼Œå°è¯•è·³å‡º å¾ªç¯â™»ï¸")
                    b_succeed=False
                    # todo ...... ä¸è€ƒè™‘å¤šæ ·æ€§çš„æ‹¼åŒ…
                    current_box2 = []
                    current_sum2 = 0
                    used_keys_num = defaultdict(int)   # è®°å½•è¿™ä¸ªæ¡¶ç”¨äº†å‡ ä¸ªå…ƒç´ 
                    for key2 in left_keys:   # å–å‡º 1ä¸ªæ¡¶
                        if b_succeed:   # åªæ‹¼ä¸€ä¸ª
                            print(f"æ”¹å˜ç­–ç•¥æ‹¼åŒ…æˆåŠŸ:âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…")
                            break
                        arr2 = self.hash_buckets[key2]
                        l_val2 = key2
                        for item2 in arr2:
                            if current_sum2 + l_val2 <= box_capacity:
                                current_box2.append(item2)
                                current_sum2 += l_val2
                                used_keys_num[key2] += 1
                                
                                if current_sum2==box_capacity:
                                    boxes.append(np.array(current_box2, dtype=self.DTYPE_SAMPLE_INFO))
                                    current_box2 = []
                                    current_sum2 = 0
                                    # åˆ é™¤å…ƒç´ 
                                    for kkey, knum in used_keys_num.items():
                                        for _ in range(knum):
                                            # self.delete_by_index(self.hash_buckets, kkey,0)
                                            self.hash_buckets[kkey] = np.delete(self.hash_buckets[kkey], 0)
                                        key_queues[kkey] = deque(enumerate(self.hash_buckets[key]))
                                    # é‡æ–°åŒæ­¥ key_queues å’Œ self.hash_buckets
                                    # key_queues = {k: deque(enumerate(self.hash_buckets[k])) 
                                    #               for k in self.hb2_keys if k in self.hash_buckets}
                                    print(f"æ”¹å˜ç­–ç•¥æ‹¼åŒ…æˆåŠŸ:âœ…âœ…âœ…âœ…âœ…{boxes[-1]}")
                                    used_keys_num = defaultdict(int)
                                    b_succeed = True
                                    break
                            else:
                                current_box2 = []
                                current_sum2 = 0
                                used_keys_num = defaultdict(int)
                                b_succeed = False
                                print(f"æ”¹å˜ç­–ç•¥æ‹¼åŒ…å¤±è´¥:âŒâŒâŒâŒâŒ")
                                break
                    pass
                else:
                    print(f"num of left_elems:{left_elems}")
    
        return boxes

    def pack_with_deletion_recursion(self, box_capacity: int = 16384) -> List[np.ndarray]:
        """é€’å½’å¤šæ ·æ€§ä¼˜å…ˆè£…ç®±ï¼šåªè¾“å‡º/åˆ é™¤æ»¡ç®±ï¼Œæ‰€æœ‰ä¸æ»¡ç®±æ··åˆé‡è£…ï¼Œç›´åˆ°åªå‰©ä¸€ä¸ªä¸æ»¡ç®±ã€‚
        ï¼ˆç”¨äºå•ç‹¬å¤„ç†  (box_capacity/key)==2^n çš„ key ï¼‰
        é€’å½’å®ç°
        """
        from collections import deque, defaultdict
        def recursive_diversity_pack(key_queues):
            boxes = []
            not_full_items = []
            print("----------- pack_with_deletion_recursion -----------")
            while any(key_queues.values()):
                current_box = []
                current_sum = 0
                used_indices = defaultdict(list)
                keys_to_try = deque(sorted(key_queues.keys()))
    
                # å¤šæ ·æ€§ä¼˜å…ˆï¼šæ¯è½®ä»ä¸åŒæ¡¶å–
                while keys_to_try and current_sum < box_capacity:
                    key = keys_to_try.popleft()
                    queue = key_queues[key]
                    if not queue:
                        continue
                    idx, item = queue[0]
                    l_val = item['l']
                    if current_sum + l_val <= box_capacity:
                        queue.popleft()
                        current_box.append((key, idx, item))
                        current_sum += l_val
                        used_indices[key].append(idx)
                        if queue:
                            keys_to_try.append(key)
    
                if current_sum == box_capacity:
                    # æ»¡ç®±ï¼Œè¾“å‡ºå¹¶è®°å½•è¦åˆ é™¤çš„ç´¢å¼•
                    boxes.append(np.array([item for _, _, item in current_box], dtype=self.DTYPE_SAMPLE_INFO))
                    for key, indices in used_indices.items():
                        # åˆ é™¤å·²ç”¨å…ƒç´ 
                        indices = sorted(indices, reverse=True)
                        for idx in indices:
                            self.hash_buckets[key] = np.delete(self.hash_buckets[key], idx)
                        # æ›´æ–° key_queues
                        key_queues[key] = deque(enumerate(self.hash_buckets[key]))
                elif current_box:
                    # ä¸æ»¡ç®±ï¼Œæš‚å­˜
                    not_full_items.extend(current_box)
    
            return boxes, not_full_items
    
        # åˆå§‹åŒ– key_queues
        key_queues = {k: deque(enumerate(self.hash_buckets[k])) for k in self.hb2_keys if k in self.hash_buckets}
        boxes, not_full_items = recursive_diversity_pack(key_queues)
    
        # æ··åˆæ‰€æœ‰ä¸æ»¡ç®±å…ƒç´ é€’å½’è£…ç®±
        while not_full_items:
            # æ··åˆæ‰€æœ‰å‰©ä½™å…ƒç´ ï¼Œé‡æ–°åˆ†æ¡¶
            mixed = defaultdict(list)
            for _, _, item in not_full_items:
                mixed[item['l']].append(item)
            key_queues = {k: deque(enumerate(np.array(v, dtype=self.DTYPE_SAMPLE_INFO))) for k, v in mixed.items()}
            new_boxes, new_not_full_items = recursive_diversity_pack(key_queues)
            boxes.extend(new_boxes)
            if not new_boxes or not new_not_full_items:
                break
            not_full_items = new_not_full_items
        return boxes, not_full_items

    def pack_large_seed_parallel_multithread(self, box_capacity: int = 16384, min_ratio: float = 0.95, 
                                           max_workers: int = None) -> List[np.ndarray]:
        """
        å¤šçº¿ç¨‹ç‰ˆæœ¬ï¼ˆå¤„ç† pack_with_deletion ä¹‹åçš„å…ƒç´ ï¼‰ï¼šå¤§ç§å­å¹¶è¡Œè£…ç®±ï¼Œå°å…ƒç´ ä½œä¸ºå…±äº«èµ„æºï¼Œå®æ—¶åˆ é™¤å…ƒç´ 
         ï¼ˆå¯¹äºä¸€ä¸ªç®±å­ä¸­çš„ç‰©å“æ•°é‡æ²¡æœ‰ä»»ä½•é™åˆ¶ï¼Œé€Ÿåº¦ä¼šæ¯”è¾ƒå¿«ä¸€ç‚¹ï¼‰
        å‚æ•°:
            box_capacity: ç®±å­å®¹é‡
            min_ratio: æœ€å°è£…è½½ç‡é˜ˆå€¼
            max_workers: æœ€å¤§çº¿ç¨‹æ•°ï¼ŒNoneæ—¶è‡ªåŠ¨è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°
        
        è¿”å›:
            List[np.ndarray]: æˆåŠŸè£…ç®±çš„ç®±å­åˆ—è¡¨
        """
        if max_workers is None:
            max_workers = min(os.cpu_count(), 8)  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°
        
        half = box_capacity // 2
        # half = 4096
        large_keys = [k for k in self.hash_buckets.keys() if k >= half]
        small_keys = [k for k in self.hash_buckets.keys() if k < half]
        
        if not large_keys:
            self._logger.warning("æ²¡æœ‰æ‰¾åˆ°å¤§ç§å­å…ƒç´ ")
            return []
    
        # 1. çº¿ç¨‹å®‰å…¨çš„å…±äº«èµ„æºç®¡ç†å™¨
        class SharedResourceManager:
            def __init__(self, hash_buckets, small_keys, large_keys):
                self.lock = threading.RLock()  # å¯é‡å…¥é”
                self.hash_buckets = hash_buckets  # ç›´æ¥å¼•ç”¨åŸå§‹å“ˆå¸Œæ¡¶
                self.small_keys = small_keys
                self.large_keys = large_keys
                
                # åˆå§‹åŒ–å¯ç”¨çš„small keys
                self.available_small_keys = sorted([
                    k for k in small_keys 
                    if k in hash_buckets and len(hash_buckets[k]) > 0
                ])
                
                # ç»Ÿè®¡ä¿¡æ¯
                self.total_processed = 0
                self.successful_boxes = 0
                
            def get_seed_item(self, seed_key: int) -> tuple:
                """çº¿ç¨‹å®‰å…¨åœ°è·å–ç§å­å…ƒç´ """
                with self.lock:
                    if (seed_key in self.hash_buckets and 
                        len(self.hash_buckets[seed_key]) > 0):
                        
                        item = self.hash_buckets[seed_key][0]
                        self.hash_buckets[seed_key] = self.hash_buckets[seed_key][1:]
                        return True, item
                    return False, None
            
            def get_item_by_key(self, target_key: int) -> tuple:
                """çº¿ç¨‹å®‰å…¨åœ°ä»æŒ‡å®škeyè·å–ä¸€ä¸ªå…ƒç´ å¹¶åˆ é™¤"""
                with self.lock:
                    if (target_key in self.hash_buckets and 
                        len(self.hash_buckets[target_key]) > 0):
                        
                        item = self.hash_buckets[target_key][0]
                        self.hash_buckets[target_key] = self.hash_buckets[target_key][1:]
                        
                        # å¦‚æœè¿™ä¸ªkeyçš„æ¡¶ç©ºäº†ï¼Œä»available_small_keysä¸­ç§»é™¤
                        if (len(self.hash_buckets[target_key]) == 0 and 
                            target_key in self.available_small_keys):
                            self.available_small_keys.remove(target_key)
                        
                        return True, item
                    return False, None
            
            def get_available_small_keys(self) -> List[int]:
                """è·å–å½“å‰å¯ç”¨çš„å°keyåˆ—è¡¨"""
                with self.lock:
                    return self.available_small_keys.copy()
            
            def update_stats(self, success: bool):
                """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
                with self.lock:
                    self.total_processed += 1
                    if success:
                        self.successful_boxes += 1
                    
            def get_stats(self) -> dict:
                """è·å–ç»Ÿè®¡ä¿¡æ¯"""
                with self.lock:
                    small_items_count = sum(
                        len(self.hash_buckets[k]) for k in self.small_keys 
                        if k in self.hash_buckets
                    )
                    large_items_count = sum(
                        len(self.hash_buckets[k]) for k in self.large_keys 
                        if k in self.hash_buckets
                    )
                    
                    return {
                        'small_items_remaining': small_items_count,
                        'large_items_remaining': large_items_count,
                        'available_small_keys': len(self.available_small_keys),
                        'total_processed': self.total_processed,
                        'successful_boxes': self.successful_boxes,
                        'success_rate': (self.successful_boxes / max(1, self.total_processed))
                    }
    
        # 2. äºŒåˆ†æŸ¥æ‰¾å‡½æ•°
        def search_for_fit_key(available_keys: List[int], remaining_capacity: int) -> int:
            """åœ¨å¯ç”¨çš„keyä¸­äºŒåˆ†æŸ¥æ‰¾æœ€å¤§èƒ½è£…å…¥çš„key"""
            if not available_keys:
                return -1
            index = bisect.bisect(available_keys, remaining_capacity)
            return -1 if index == 0 else (index - 1)
    
        # 3. å•ä¸ªç§å­çš„è£…ç®±å‡½æ•°
        def pack_single_seed(seed_key: int, shared_manager: SharedResourceManager, 
                            thread_id: int) -> tuple:
            """ä¸ºå•ä¸ªç§å­è¿›è¡Œè£…ç®±"""
            try:
                # è·å–ç§å­å…ƒç´ 
                success, seed_item = shared_manager.get_seed_item(seed_key)
                if not success:
                    return False, None, thread_id, 0, "æ— å¯ç”¨ç§å­"
                
                current_box = [seed_item]
                remaining_capacity = box_capacity - seed_key
                items_added = 1
            
                # è´ªå¿ƒè£…ç®±ï¼šä¼˜å…ˆè£…å…¥å¤§çš„å…ƒç´ 
                max_iterations = 1000  # é˜²æ­¢æ­»å¾ªç¯
                iteration = 0
                
                while remaining_capacity > 0 and iteration < max_iterations:
                    iteration += 1
                    
                    # è·å–å½“å‰å¯ç”¨çš„å°key
                    available_keys = shared_manager.get_available_small_keys()
                    if not available_keys:
                        break
                    
                    # äºŒåˆ†æŸ¥æ‰¾æœ€å¤§å¯è£…å…¥çš„key
                    best_key_index = search_for_fit_key(available_keys, remaining_capacity)
                    if best_key_index == -1:
                        break
                    
                    best_key = available_keys[best_key_index]
                    
                    # å°è¯•ä»è¯¥keyè·å–å…ƒç´ 
                    success, item = shared_manager.get_item_by_key(best_key)
                    if not success:
                        continue  # è¿™ä¸ªkeyå·²ç»è¢«å…¶ä»–çº¿ç¨‹ç”¨å®Œï¼Œé‡è¯•
                    
                    current_box.append(item)
                    remaining_capacity -= best_key
                    items_added += 1
                    
                    # å¦‚æœè£…æ»¡å°±åœæ­¢
                    if remaining_capacity == 0:
                        break
                
                # æ£€æŸ¥è£…è½½ç‡
                current_capacity = box_capacity - remaining_capacity
                is_successful = current_capacity >= min_ratio * box_capacity
                
                result_box = current_box if is_successful else None
                load_ratio = current_capacity / box_capacity
                
                return (is_successful, result_box, thread_id, current_capacity, 
                       f"è£…è½½ç‡:{load_ratio:.1%}, ç‰©å“æ•°:{items_added}")
                
            except Exception as e:
                return False, None, thread_id, 0, f"è£…ç®±å¼‚å¸¸: {str(e)}"
    
        # 4. å‡†å¤‡æ‰€æœ‰å¤§ç§å­ä»»åŠ¡
        seed_tasks = []
        total_large_items = 0
        
        for key in large_keys:
            if key in self.hash_buckets:
                count = len(self.hash_buckets[key])
                total_large_items += count
                # ä¸ºæ¯ä¸ªå¤§å…ƒç´ åˆ›å»ºä¸€ä¸ªè£…ç®±ä»»åŠ¡
                for _ in range(count):
                    seed_tasks.append(key)
    
        if not seed_tasks:
            self._logger.warning("æ²¡æœ‰å¯ç”¨çš„å¤§ç§å­å…ƒç´ ")
            return []
    
        # 5. åˆå§‹åŒ–å…±äº«èµ„æºç®¡ç†å™¨
        shared_manager = SharedResourceManager(self.hash_buckets, small_keys, large_keys)
        initial_stats = shared_manager.get_stats()
        
        self._logger.info(f"å¼€å§‹å¤šçº¿ç¨‹è£…ç®±:")
        self._logger.info(f"  ğŸŒ± å¤§ç§å­ä»»åŠ¡: {len(seed_tasks)} ä¸ª")
        self._logger.info(f"  ğŸ”§ çº¿ç¨‹æ•°: {max_workers}")
        self._logger.info(f"  ğŸ“¦ ç›®æ ‡å®¹é‡: {box_capacity}")
        self._logger.info(f"  ğŸ“Š æœ€å°è£…è½½ç‡: {min_ratio:.1%}")
        self._logger.info(f"  ğŸ—‚ï¸ å°å…ƒç´ : {initial_stats['small_items_remaining']} ä¸ª")
    
        # 6. å¤šçº¿ç¨‹æ‰§è¡Œ
        output_boxes = []
        failed_reasons = defaultdict(int)
        start_time = time.time()
    
        with ThreadPoolExecutor(max_workers=max_workers, 
                               thread_name_prefix="PackWorker") as executor:
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {}
            for i, seed_key in enumerate(seed_tasks):
                future = executor.submit(pack_single_seed, seed_key, shared_manager, i)
                future_to_task[future] = (seed_key, i)
            
            # å¤„ç†ç»“æœ
            with tqdm(total=len(seed_tasks), unit='seed', 
                     desc=f'å¤šçº¿ç¨‹è£…ç®±', dynamic_ncols=True) as pbar:
                
                for future in as_completed(future_to_task):
                    seed_key, task_id = future_to_task[future]
                    
                    try:
                        success, box, thread_id, capacity, info = future.result(timeout=30)
                        
                        shared_manager.update_stats(success)
                        
                        if success and box is not None:
                            output_boxes.append(np.array(box, dtype=self.DTYPE_SAMPLE_INFO))
                        else:
                            failed_reasons[info] += 1
                        
                        pbar.update(1)
                        
                        # æ¯50ä¸ªä»»åŠ¡æ›´æ–°ä¸€æ¬¡æè¿°
                        if pbar.n % 50 == 0:
                            current_stats = shared_manager.get_stats()
                            pbar.set_description(
                                f'è£…ç®±è¿›åº¦(æˆåŠŸ:{current_stats["successful_boxes"]}, '
                                f'æˆåŠŸç‡:{current_stats["success_rate"]:.1%}, '
                                f'å‰©ä½™å°å…ƒç´ :{current_stats["small_items_remaining"]})'
                            )
                            
                    except Exception as e:
                        self._logger.error(f"ä»»åŠ¡ {task_id} (ç§å­key={seed_key}) æ‰§è¡Œå¤±è´¥: {e}")
                        failed_reasons[f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"] += 1
                        pbar.update(1)
    
        end_time = time.time()
        
        # 7. è¾“å‡ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        final_stats = shared_manager.get_stats()
        
        if output_boxes:
            total_items = sum(len(box) for box in output_boxes)
            avg_items_per_box = total_items / len(output_boxes)
            total_capacity_used = len(output_boxes) * box_capacity
            
            self._logger.info(f"å¤šçº¿ç¨‹è£…ç®±å®Œæˆ:")
            self._logger.info(f"  â±ï¸  æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
            self._logger.info(f"  ğŸ“¦ æˆåŠŸç®±å­: {len(output_boxes)}")
            self._logger.info(f"  ğŸ“Š æ€»æˆåŠŸç‡: {final_stats['success_rate']:.2%}")
            self._logger.info(f"  ğŸ“ˆ å¹³å‡æ¯ç®±ç‰©å“æ•°: {avg_items_per_box:.1f}")
            self._logger.info(f"  ğŸ’¾ æ€»è®¡ä½¿ç”¨ç‰©å“: {total_items}")
            self._logger.info(f"  ğŸ”— å‰©ä½™å°å…ƒç´ : {final_stats['small_items_remaining']}")
            self._logger.info(f"  ğŸ”‘ å‰©ä½™å°keyæ•°: {final_stats['available_small_keys']}")
            
            if failed_reasons:
                self._logger.info(f"  âŒ å¤±è´¥åŸå› ç»Ÿè®¡:")
                for reason, count in failed_reasons.items():
                    self._logger.info(f"     {reason}: {count}æ¬¡")
        else:
            self._logger.warning("æ²¡æœ‰æˆåŠŸè£…ç®±ä»»ä½•ç‰©å“")
            self._logger.info(f"å¤±è´¥åŸå› : {dict(failed_reasons)}")
    
        return output_boxes

    def pack_with_min_items_constraint_multithread(self, box_capacity: int = 16384, 
                                                 min_items: int = 10, min_ratio: float = 0.95,
                                                 max_workers: int = None) -> List[np.ndarray]:
        """
        å¤šçº¿ç¨‹å¤šçº¦æŸè£…ç®±ï¼šå®¹é‡çº¦æŸ + æœ€å°ç‰©å“æ•°é‡çº¦æŸ
        ï¼ˆå¯¹äºæ¯ä¸ªç®±å­å†…çš„æœ€å°‘ç‰©å“æ•°å¢åŠ é™åˆ¶,ä¿è¯åç»­çš„ attn æ—¶é—´å°½é‡æ¥è¿‘ï¼‰
        å‚æ•°:
            box_capacity: ç®±å­å®¹é‡
            min_items: æ¯ç®±æœ€å°‘ç‰©å“æ•°é‡
            min_ratio: æœ€å°è£…è½½ç‡é˜ˆå€¼
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
        
        è¿”å›:
            List[np.ndarray]: æ»¡è¶³æ‰€æœ‰çº¦æŸçš„ç®±å­åˆ—è¡¨
        """
        if max_workers is None:
            max_workers = min(os.cpu_count(), 6)  # çº¦æŸé—®é¢˜è®¡ç®—é‡å¤§ï¼Œå‡å°‘çº¿ç¨‹æ•°
        
        half = box_capacity // 2
        # half = 4096
        print(f"ç§å­ç­›é€‰æ¡ä»¶å‚æ•° half:{half}")
        large_keys = [k for k in self.hash_buckets.keys() if k >= half]
        small_keys = [k for k in self.hash_buckets.keys() if k < half]
        
        if not large_keys:
            self._logger.warning("æ²¡æœ‰æ‰¾åˆ°å¤§ç§å­å…ƒç´ ")
            return []
    
        # 1. ç§å­æ½œåŠ›è¯„ä¼°å™¨
        class SeedPotentialAnalyzer:
            def __init__(self, hash_buckets, small_keys, box_capacity, min_items):
                self.hash_buckets = hash_buckets
                self.small_keys = sorted(small_keys)
                self.box_capacity = box_capacity
                self.min_items = min_items
                
            def calculate_potential(self, seed_key: int) -> float:
                """è®¡ç®—ç§å­çš„è£…ç®±æˆåŠŸæ½œåŠ›"""
                remaining_capacity = self.box_capacity - seed_key
                
                if remaining_capacity <= 0:
                    return 0.0
                
                # ç»Ÿè®¡å°å…ƒç´ çš„åˆ†å¸ƒ
                available_small_items = sum(
                    len(self.hash_buckets[k]) for k in self.small_keys 
                    if k in self.hash_buckets
                )
                
                if available_small_items == 0:
                    return 0.0
                
                # ä¼°ç®—èƒ½è£…å…¥çš„ç‰©å“æ•°é‡ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
                min_small_key = min(self.small_keys) if self.small_keys else remaining_capacity
                max_possible_items = remaining_capacity // min_small_key
                
                # è€ƒè™‘å®é™…å¯ç”¨æ€§ï¼ˆä¸æ˜¯æ‰€æœ‰å°keyéƒ½æœ‰å…ƒç´ ï¼‰
                practical_items = min(max_possible_items, available_small_items // 2)
                total_items = practical_items + 1  # +1 for seed
                
                # æ½œåŠ›è¯„åˆ†
                count_score = min(total_items / self.min_items, 1.0) if self.min_items > 0 else 1.0
                capacity_score = seed_key / self.box_capacity
                diversity_score = len([k for k in self.small_keys if k <= remaining_capacity]) / len(self.small_keys) if self.small_keys else 0
                
                return count_score * 0.5 + capacity_score * 0.3 + diversity_score * 0.2

        # 2. å¢å¼ºçš„å…±äº«èµ„æºç®¡ç†å™¨
        class EnhancedSharedManager:
            def __init__(self, hash_buckets, small_keys, large_keys):
                self.lock = threading.RLock()
                self.hash_buckets = hash_buckets
                self.small_keys = sorted(small_keys)
                self.large_keys = large_keys
                
                # ç»´æŠ¤å¯ç”¨keyçš„ç»Ÿè®¡ä¿¡æ¯
                self.key_stats = {}
                self._update_key_stats()
                
                # æ€§èƒ½ç»Ÿè®¡
                self.stats = {
                    'total_attempts': 0,
                    'successful_boxes': 0,
                    'failed_by_count': 0,
                    'failed_by_ratio': 0,
                    'failed_by_capacity': 0
                }
            
            def _update_key_stats(self):
                """æ›´æ–°keyçš„ç»Ÿè®¡ä¿¡æ¯"""
                self.key_stats = {}
                for k in self.small_keys:
                    if k in self.hash_buckets and len(self.hash_buckets[k]) > 0:
                        self.key_stats[k] = len(self.hash_buckets[k])
                        
            def get_seed_item(self, seed_key: int) -> Tuple[bool, Optional[np.record]]:
                """è·å–ç§å­å…ƒç´ """
                with self.lock:
                    if (seed_key in self.hash_buckets and 
                        len(self.hash_buckets[seed_key]) > 0):
                        item = self.hash_buckets[seed_key][0]
                        self.hash_buckets[seed_key] = self.hash_buckets[seed_key][1:]
                        return True, item
                    return False, None
            
            def get_item_by_key(self, target_key: int) -> Tuple[bool, Optional[np.record]]:
                """è·å–æŒ‡å®škeyçš„å…ƒç´ """
                with self.lock:
                    if (target_key in self.hash_buckets and 
                        len(self.hash_buckets[target_key]) > 0):
                        item = self.hash_buckets[target_key][0]
                        self.hash_buckets[target_key] = self.hash_buckets[target_key][1:]
                        
                        # æ›´æ–°ç»Ÿè®¡
                        if target_key in self.key_stats:
                            self.key_stats[target_key] -= 1
                            if self.key_stats[target_key] <= 0:
                                del self.key_stats[target_key]
                        
                        return True, item
                    return False, None
            
            def get_available_keys_with_counts(self) -> Dict[int, int]:
                """è·å–å¯ç”¨keyåŠå…¶å…ƒç´ æ•°é‡"""
                with self.lock:
                    return self.key_stats.copy()
            
            def rollback_items(self, items_to_rollback: List[Tuple[int, np.record]]):
                """å›æ»šå¤±è´¥è£…ç®±çš„å…ƒç´ """
                with self.lock:
                    for key, item in reversed(items_to_rollback):  # é€†åºå›æ»š
                        self.hash_buckets[key] = np.insert(self.hash_buckets[key], 0, item)
                        # æ›´æ–°ç»Ÿè®¡
                        if key in self.small_keys:
                            self.key_stats[key] = self.key_stats.get(key, 0) + 1
            
            def update_stats(self, result_type: str):
                """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
                with self.lock:
                    self.stats['total_attempts'] += 1
                    if result_type in self.stats:
                        self.stats[result_type] += 1
            
            def get_current_stats(self) -> Dict:
                """è·å–å½“å‰ç»Ÿè®¡"""
                with self.lock:
                    total_small_items = sum(
                        len(self.hash_buckets[k]) for k in self.small_keys 
                        if k in self.hash_buckets
                    )
                    return {
                        **self.stats,
                        'remaining_small_items': total_small_items,
                        'available_key_types': len(self.key_stats)
                    }

        # 3. æ™ºèƒ½è£…ç®±ç­–ç•¥
        def is_feasible_quick_check(remaining_capacity: int, current_items: int, 
                                   available_keys: Dict[int, int], min_items: int) -> bool:
            """å¿«é€Ÿå¯è¡Œæ€§æ£€æŸ¥"""
            if current_items >= min_items:
                return True
                
            needed_items = min_items - current_items
            if not available_keys:
                return False
            
            # è´ªå¿ƒä¼°ç®—ï¼šä¼˜å…ˆä½¿ç”¨å°key
            sorted_keys = sorted(available_keys.keys())
            possible_items = 0
            remaining_cap = remaining_capacity
            
            for key in sorted_keys:
                if remaining_cap <= 0:
                    break
                max_from_this_key = min(remaining_cap // key, available_keys[key])
                possible_items += max_from_this_key
                remaining_cap -= max_from_this_key * key
                
                if possible_items >= needed_items:
                    return True
                    
            return False
    
        def select_optimal_key(strategy: str, available_keys: Dict[int, int], 
                              remaining_capacity: int, current_items: int, min_items: int) -> Optional[int]:
            """æ ¹æ®ç­–ç•¥é€‰æ‹©æœ€ä¼˜key"""
            suitable_keys = [k for k in available_keys.keys() 
                            if k <= remaining_capacity and available_keys[k] > 0]
            if not suitable_keys:
                return None
            
            if strategy == "prioritize_count":
                # ä¼˜å…ˆæ•°é‡ï¼šé€‰æ‹©æœ€å°çš„key
                return min(suitable_keys)
            elif strategy == "prioritize_capacity":
                # ä¼˜å…ˆå®¹é‡ï¼šé€‰æ‹©æœ€å¤§çš„key
                return max(suitable_keys)
            elif strategy == "balanced":
                # å¹³è¡¡ç­–ç•¥ï¼šé€‰æ‹©ä¸­ç­‰å¤§å°ï¼Œä½†è€ƒè™‘å¯ç”¨æ•°é‡
                suitable_keys.sort()
                # å€¾å‘äºé€‰æ‹©æœ‰è¾ƒå¤šå¯ç”¨å…ƒç´ çš„key
                key_scores = [(k, available_keys[k] * (remaining_capacity / k)) for k in suitable_keys]
                key_scores.sort(key=lambda x: x[1], reverse=True)
                return key_scores[0][0]
            else:
                return suitable_keys[0]

        # 4. æ ¸å¿ƒè£…ç®±å‡½æ•°
        def pack_single_seed_with_constraints(seed_key: int, shared_manager: EnhancedSharedManager, 
                                            thread_id: int) -> Tuple:
            """å¸¦çº¦æŸçš„å•ç§å­è£…ç®±"""
            try:
                # è·å–ç§å­
                success, seed_item = shared_manager.get_seed_item(seed_key)
                if not success:
                    shared_manager.update_stats('failed_by_capacity')
                    return False, None, thread_id, 0, 0, "æ— å¯ç”¨ç§å­"
                
                current_box = [seed_item]
                used_items = [(seed_key, seed_item)]  # ç”¨äºå›æ»š
                remaining_capacity = box_capacity - seed_key
                items_count = 1
                
                max_iterations = min_items * 16  # é˜²æ­¢æ— é™å¾ªç¯
                iteration = 0
                
                # è£…ç®±ä¸»å¾ªç¯
                while (remaining_capacity > 0 and 
                       items_count < min_items * 8 and  # å…è®¸è¶…è¿‡æœ€å°æ•°é‡
                       iteration < max_iterations):
                    
                    iteration += 1
                    available_keys = shared_manager.get_available_keys_with_counts()
                    
                    # å¿«é€Ÿå¯è¡Œæ€§æ£€æŸ¥
                    if (items_count < min_items and 
                        not is_feasible_quick_check(remaining_capacity, items_count, 
                                                  available_keys, min_items)):
                        # æ— æ³•è¾¾åˆ°æœ€å°ç‰©å“æ•°ï¼Œæå‰é€€å‡º
                        shared_manager.rollback_items(used_items)
                        shared_manager.update_stats('failed_by_count')
                        return False, None, thread_id, 0, items_count, f"æ— æ³•è¾¾åˆ°{min_items}ä¸ªç‰©å“"
                    
                    # åŠ¨æ€ç­–ç•¥é€‰æ‹©
                    if items_count < min_items * 0.8:
                        strategy = "prioritize_count"
                    elif items_count < min_items:
                        strategy = "balanced"
                    else:
                        strategy = "prioritize_capacity"
                    
                    # é€‰æ‹©ä¸‹ä¸€ä¸ªkey
                    target_key = select_optimal_key(strategy, available_keys, 
                                                  remaining_capacity, items_count, min_items)
                    if target_key is None:
                        break
                    
                    # è·å–å…ƒç´ 
                    success, item = shared_manager.get_item_by_key(target_key)
                    if not success:
                        continue  # è¯¥keyå·²è¢«å…¶ä»–çº¿ç¨‹ç”¨å®Œ
                    
                    current_box.append(item)
                    used_items.append((target_key, item))
                    remaining_capacity -= target_key
                    items_count += 1
                    
                    # å¦‚æœè¾¾åˆ°å®Œç¾è£…è½½ï¼Œå¯ä»¥æå‰ç»“æŸ
                    if remaining_capacity == 0 and items_count >= min_items:
                        break
                
                # æ£€æŸ¥æ‰€æœ‰çº¦æŸ
                current_capacity = box_capacity - remaining_capacity
                load_ratio = current_capacity / box_capacity
                
                meets_count = items_count >= min_items
                meets_ratio = load_ratio >= min_ratio
                meets_capacity = remaining_capacity >= 0
                
                success = meets_count and meets_ratio and meets_capacity
                
                if success:
                    shared_manager.update_stats('successful_boxes')
                    return True, current_box, thread_id, current_capacity, items_count, f"æˆåŠŸï¼š{items_count}ä¸ªç‰©å“ï¼Œè£…è½½ç‡{load_ratio:.1%}"
                else:
                    # è£…ç®±å¤±è´¥ï¼Œå›æ»š
                    shared_manager.rollback_items(used_items)
                    if not meets_count:
                        shared_manager.update_stats('failed_by_count')
                        reason = f"ç‰©å“æ•°ä¸è¶³ï¼š{items_count}<{min_items}"
                    elif not meets_ratio:
                        shared_manager.update_stats('failed_by_ratio')
                        reason = f"è£…è½½ç‡ä¸è¶³ï¼š{load_ratio:.1%}<{min_ratio:.1%}"
                    else:
                        shared_manager.update_stats('failed_by_capacity')
                        reason = "å®¹é‡çº¦æŸå¤±è´¥"
                    
                    return False, None, thread_id, current_capacity, items_count, reason
                    
            except Exception as e:
                shared_manager.update_stats('failed_by_capacity')
                return False, None, thread_id, 0, 0, f"è£…ç®±å¼‚å¸¸: {str(e)}"
    
        # 5. ç§å­é¢„å¤„ç†å’Œç­›é€‰
        analyzer = SeedPotentialAnalyzer(self.hash_buckets, small_keys, box_capacity, min_items)
        
        # æ”¶é›†å¹¶è¯„ä¼°æ‰€æœ‰ç§å­
        seed_candidates = []
        for key in large_keys:
            if key in self.hash_buckets:
                potential = analyzer.calculate_potential(key)
                count = len(self.hash_buckets[key])
                for _ in range(count):
                    seed_candidates.append((key, potential))  # ç¡®ä¿æ˜¯å…ƒç»„
        if not seed_candidates:
            self._logger.warning("æ²¡æœ‰å¯ç”¨çš„ç§å­å€™é€‰")
            return []
        
        # æŒ‰æ½œåŠ›æ’åºï¼Œåªå¤„ç†é«˜æ½œåŠ›ç§å­
        seed_candidates.sort(key=lambda x: x[1], reverse=True)
        potential_threshold = 0.2  # åªå¤„ç†æ½œåŠ›>0.2çš„ç§å­
        # high_potential_seeds = [seed for seed, potential in seed_candidates if potential > potential_threshold]
        # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†ç­›é€‰é€»è¾‘
        high_potential_candidates = [(seed, potential) for seed, potential in seed_candidates 
                                if potential > potential_threshold]

        # æœ€åä¿åº•ï¼šè‡³å°‘ä¿ç•™å‰50%çš„ç§å­
        if len(high_potential_candidates) < len(seed_candidates) * 0.5:
            mid_point = len(seed_candidates) // 2
            high_potential_candidates = seed_candidates[:mid_point]
        
        # ä¿®å¤ï¼šæ­£ç¡®æå–ç§å­åˆ—è¡¨
        selected_seeds = [seed for seed, potential in high_potential_candidates]
        
        self._logger.info(f"ç§å­ç­›é€‰å®Œæˆ:")
        self._logger.info(f"  ğŸ“Š æ€»ç§å­æ•°: {len(seed_candidates)}")
        self._logger.info(f"  ğŸ¯ ç­›é€‰å: {len(selected_seeds)}")
        self._logger.info(f"  ğŸš€ ç­›é€‰ç‡: {len(selected_seeds)/len(seed_candidates):.1%}")
        self._logger.info(f"  ğŸ”§ çº¿ç¨‹æ•°: {max_workers}")
        self._logger.info(f"  ğŸ“¦ çº¦æŸ: å®¹é‡â‰¥{min_ratio:.0%}, ç‰©å“â‰¥{min_items}ä¸ª")
    
        # 6. åˆå§‹åŒ–å…±äº«ç®¡ç†å™¨
        shared_manager = EnhancedSharedManager(self.hash_buckets, small_keys, large_keys)
        initial_stats = shared_manager.get_current_stats()
        
        self._logger.info(f"åˆå§‹èµ„æºçŠ¶æ€:")
        self._logger.info(f"  ğŸ—‚ï¸ å°å…ƒç´ æ€»æ•°: {initial_stats['remaining_small_items']}")
        self._logger.info(f"  ğŸ”‘ å¯ç”¨å°keyç§ç±»: {initial_stats['available_key_types']}")

        # 7. å¤šçº¿ç¨‹æ‰§è¡Œè£…ç®±
        output_boxes = []
        detailed_results = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=max_workers, 
                               thread_name_prefix="ConstraintPack") as executor:
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_seed = {}
            for i, seed_key in enumerate(selected_seeds):
                future = executor.submit(pack_single_seed_with_constraints, seed_key, shared_manager, i)
                future_to_seed[future] = (seed_key, i)
            
            # å¤„ç†ç»“æœ
            with tqdm(total=len(selected_seeds), unit='seed', 
                     desc='å¤šçº¦æŸè£…ç®±', dynamic_ncols=True) as pbar:
                
                completed_tasks = 0
                for future in as_completed(future_to_seed):
                    seed_key, task_id = future_to_seed[future]
                    
                    try:
                        success, box, thread_id, capacity, item_count, info = future.result(timeout=60)
                        
                        if success and box is not None:
                            output_boxes.append(np.array(box, dtype=self.DTYPE_SAMPLE_INFO))
                        
                        detailed_results.append({
                            'seed_key': seed_key,
                            'success': success,
                            'capacity': capacity,
                            'item_count': item_count,
                            'info': info,
                            'thread_id': thread_id
                        })
    
                        completed_tasks += 1
                        pbar.update(1)
                        
                        # æ¯100ä¸ªä»»åŠ¡æ›´æ–°ä¸€æ¬¡è¿›åº¦æè¿°
                        if completed_tasks % 100 == 0:
                            current_stats = shared_manager.get_current_stats()
                            success_rate = current_stats['successful_boxes'] / max(1, current_stats['total_attempts'])
                            pbar.set_description(
                                f'å¤šçº¦æŸè£…ç®±(æˆåŠŸ:{current_stats["successful_boxes"]}, '
                                f'æˆåŠŸç‡:{success_rate:.1%}, '
                                f'å‰©ä½™:{current_stats["remaining_small_items"]})'
                            )
                            
                    except Exception as e:
                        self._logger.error(f"ä»»åŠ¡ {task_id} (ç§å­={seed_key}) å¤±è´¥: {e}")
                        detailed_results.append({
                            'seed_key': seed_key,
                            'success': False,
                            'capacity': 0,
                            'item_count': 0,
                            'info': f"ä»»åŠ¡å¼‚å¸¸: {str(e)}",
                            'thread_id': -1
                        })
                        pbar.update(1)
    
        end_time = time.time()

        # 8. è¯¦ç»†ç»Ÿè®¡åˆ†æ
        final_stats = shared_manager.get_current_stats()
        
        # æŒ‰å¤±è´¥åŸå› åˆ†ç±»
        failure_analysis = defaultdict(int)
        success_details = []
        
        for result in detailed_results:
            if result['success']:
                success_details.append(result)
            else:
                # ç®€åŒ–å¤±è´¥åŸå› 
                info = result['info']
                if 'ç‰©å“æ•°ä¸è¶³' in info:
                    failure_analysis['ç‰©å“æ•°é‡ä¸è¶³'] += 1
                elif 'è£…è½½ç‡ä¸è¶³' in info:
                    failure_analysis['è£…è½½ç‡ä¸è¶³'] += 1
                elif 'æ— å¯ç”¨ç§å­' in info:
                    failure_analysis['ç§å­è€—å°½'] += 1
                elif 'æ— æ³•è¾¾åˆ°' in info:
                    failure_analysis['å¯è¡Œæ€§æ£€æŸ¥å¤±è´¥'] += 1
                else:
                    failure_analysis['å…¶ä»–åŸå› '] += 1

        # 9. è¾“å‡ºè¯¦ç»†æŠ¥å‘Š
        if output_boxes:
            # æˆåŠŸè£…ç®±çš„ç»Ÿè®¡
            total_items_packed = sum(len(box) for box in output_boxes)
            avg_items_per_box = total_items_packed / len(output_boxes)
            capacities = [result['capacity'] for result in success_details]
            avg_capacity = sum(capacities) / len(capacities) if capacities else 0
            avg_load_ratio = avg_capacity / box_capacity
            
            item_counts = [result['item_count'] for result in success_details]
            min_items_in_box = min(item_counts) if item_counts else 0
            max_items_in_box = max(item_counts) if item_counts else 0
            
            self._logger.info(f"ğŸ‰ å¤šçº¦æŸè£…ç®±å®Œæˆ!")
            self._logger.info(f"ğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
            self._logger.info(f"  â±ï¸  æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
            self._logger.info(f"  ğŸ¯ å¤„ç†ç§å­: {len(selected_seeds)}")
            self._logger.info(f"  ğŸ“¦ æˆåŠŸç®±å­: {len(output_boxes)}")
            self._logger.info(f"  ğŸ“ˆ æ€»æˆåŠŸç‡: {len(output_boxes)/len(selected_seeds):.2%}")
            
            self._logger.info(f"ğŸ“¦ è£…ç®±è´¨é‡:")
            self._logger.info(f"  ğŸ“Š å¹³å‡è£…è½½ç‡: {avg_load_ratio:.1%}")
            self._logger.info(f"  ğŸ”¢ å¹³å‡ç‰©å“æ•°: {avg_items_per_box:.1f}")
            self._logger.info(f"  ğŸ“‰ ç‰©å“æ•°èŒƒå›´: {min_items_in_box}-{max_items_in_box}")
            self._logger.info(f"  ğŸ’¾ æ€»æ‰“åŒ…ç‰©å“: {total_items_packed}")
            
            self._logger.info(f"ğŸ”— å‰©ä½™èµ„æº:")
            self._logger.info(f"  ğŸ—‚ï¸ å°å…ƒç´ : {final_stats['remaining_small_items']}")
            self._logger.info(f"  ğŸ”‘ å¯ç”¨keyç±»å‹: {final_stats['available_key_types']}")
            
            if failure_analysis:
                self._logger.info(f"âŒ å¤±è´¥åˆ†æ:")
                for reason, count in failure_analysis.items():
                    percentage = count / len(selected_seeds) * 100
                    self._logger.info(f"     {reason}: {count}æ¬¡ ({percentage:.1f}%)")
        else:
            self._logger.warning("âš ï¸  æ²¡æœ‰æˆåŠŸè£…ç®±ä»»ä½•ç‰©å“!")
            self._logger.info(f"å¤±è´¥åŸå› åˆ†å¸ƒ: {dict(failure_analysis)}")
            self._logger.info(f"å»ºè®®:")
            self._logger.info(f"  1. é™ä½ min_items (å½“å‰: {min_items})")
            self._logger.info(f"  2. é™ä½ min_ratio (å½“å‰: {min_ratio})")
            self._logger.info(f"  3. æ£€æŸ¥æ•°æ®åˆ†å¸ƒæ˜¯å¦åˆç†")
    
        return output_boxes

    # '''
    #     # 10. å°†å‡½æ•°ç»‘å®šåˆ°ç±»
    #     # HashBucketProcessor.pack_with_min_items_constraint_multithread = pack_with_min_items_constraint_multithread
        
    #     # 11. ä½¿ç”¨ç¤ºä¾‹
    #     def demo_constrained_packing():
    #         """æ¼”ç¤ºå¤šçº¦æŸè£…ç®±çš„ä½¿ç”¨"""
            
    #         # åˆ›å»ºå¤„ç†å™¨
    #         processor = HashBucketProcessor("data.txt")
    #         processor.build_buckets()
    #         processor.find_items(16384)
            
    #         print("=== åŸå§‹è£…ç®±ï¼ˆæ— ç‰©å“æ•°é‡çº¦æŸï¼‰ ===")
    #         boxes_original = processor.pack_large_seed_parallel_multithread(
    #             box_capacity=16384,
    #             min_ratio=0.95,
    #             max_workers=4
    #         )
    #         print(f"åŸå§‹æ–¹æ³•æˆåŠŸç®±å­: {len(boxes_original)}")
            
    #         print("\n=== å¤šçº¦æŸè£…ç®±ï¼ˆè‡³å°‘10ä¸ªç‰©å“ï¼‰ ===")
    #         boxes_constrained = processor.pack_with_min_items_constraint_multithread(
    #             box_capacity=16384,
    #             min_items=10,
    #             min_ratio=0.95,
    #             max_workers=4
    #         )
    #         print(f"çº¦æŸæ–¹æ³•æˆåŠŸç®±å­: {len(boxes_constrained)}")
            
    #         # å¯¹æ¯”åˆ†æ
    #         if boxes_constrained:
    #             avg_items_constrained = sum(len(box) for box in boxes_constrained) / len(boxes_constrained)
    #             print(f"çº¦æŸæ–¹æ³•å¹³å‡æ¯ç®±ç‰©å“æ•°: {avg_items_constrained:.1f}")
            
    #         return boxes_original, boxes_constrained
        
    #     # å¦‚æœæƒ³ç›´æ¥è¿è¡Œæ¼”ç¤º
    #     if __name__ == "__main__":
    #         # demo_constrained_packing()
    #         pass
    # '''
    def pack_with_flexible_seeds(self, box_capacity: int = 16384,
                               seed_strategy: str = "auto",
                               seed_params: dict = None,
                               min_items: int = 10, min_ratio: float = 0.95,
                               max_workers: int = None) -> List[np.ndarray]:
        """
        è‡ªå®šä¹‰ç§å­é€‰æ‹©ç­–ç•¥ + èƒŒåŒ…å…ƒç´ æ•°é™åˆ¶ + è¾“å‡ºèƒŒåŒ…æœ€å°å®¹é‡
        
        å‚æ•°:
            seed_strategy: ç§å­ç­–ç•¥
                - "auto": è‡ªåŠ¨ä½¿ç”¨ box_capacity // 2
                - "custom_half": ä½¿ç”¨è‡ªå®šä¹‰çš„halfå€¼
                - "specified_keys": ä½¿ç”¨æŒ‡å®šçš„keyåˆ—è¡¨
                - "size_range": ä½¿ç”¨å¤§å°èŒƒå›´ç­›é€‰
                - "top_n": ä½¿ç”¨æœ€å¤§çš„Nä¸ªkeysä½œä¸ºç§å­
                - "capacity_ratio": æŒ‡å®šå ç”¨ capacity çš„ç™¾åˆ†æ¯”çš„
            seed_params: ç­–ç•¥å‚æ•°å­—å…¸
                - 
        """
        if max_workers is None:
            max_workers = min(os.cpu_count(), 6)
        
        if seed_params is None:
            seed_params = {}
        
        # ğŸ¯ æ ¹æ®ç­–ç•¥ç”Ÿæˆç§å­
        if seed_strategy == "auto":
            half = box_capacity // 2
            large_keys = [k for k in self.hash_buckets.keys() if k >= half]
            
        elif seed_strategy == "custom_half":
            custom_half = seed_params.get("half", box_capacity // 3)
            # max_elems = seed_params.get("n_max", None)         # æ¯ä¸ª key ä¸­æœ€å¤šå–å‡ºçš„ç§å­æ•°Ã·
            large_keys = [k for k in self.hash_buckets.keys() if k >= custom_half]
            
        elif seed_strategy == "specified_keys":
            specified_keys = seed_params.get("keys", [])
            large_keys = [k for k in specified_keys if k in self.hash_buckets]
            
        elif seed_strategy == "size_range":
            min_size = seed_params.get("min_size", box_capacity // 3)
            max_size = seed_params.get("max_size", box_capacity)
            large_keys = [k for k in self.hash_buckets.keys() 
                         if min_size <= k <= max_size]
            
        elif seed_strategy == "top_n":
            n = seed_params.get("n", 5)
            available_keys = sorted(self.hash_buckets.keys(), reverse=True)
            large_keys = available_keys[:n]
            
        elif seed_strategy == "capacity_ratio":
            min_ratio = seed_params.get("min_ratio", 0.3)  # è‡³å°‘30%å®¹é‡
            max_ratio = seed_params.get("max_ratio", 1.0)  # æœ€å¤š100%å®¹é‡
            min_size = int(box_capacity * min_ratio)
            max_size = int(box_capacity * max_ratio)
            large_keys = [k for k in self.hash_buckets.keys() 
                         if min_size <= k <= max_size]

        # # åˆ©ç”¨åˆ†ä½æ•° Quartiles è¿›è¡Œç§å­çš„é€‰æ‹©(Q1,Q2,Q3)
        # elif seed_strategy == "quartiles":
        #     q_n = seed_params.get("q_n", 3)  # è‡³å°‘30%å®¹é‡
        #     elems_max_num = seed_params.get("max_num", 20)  # æ¯ä¸€ä¸ª key é‡Œé¢å–å‡ºä½œä¸ºç§å­çš„çš„æœ€å¤§å…ƒç´ ä¸ªæ•°
        #     pass
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç§å­ç­–ç•¥: {seed_strategy}")
        
        # ç”Ÿæˆå°å…ƒç´ åˆ—è¡¨
        small_keys = [k for k in self.hash_buckets.keys() if k not in large_keys]
        
        # ç­–ç•¥ä¿¡æ¯
        self._logger.info(f"ç§å­ç­–ç•¥: {seed_strategy}")
        self._logger.info(f"ç­–ç•¥å‚æ•°: {seed_params}")
        self._logger.info(f"  ğŸŒ± ç§å­keys(max): {large_keys[-1]}")
        self._logger.info(f"  ğŸ”§ å¡«å……keysæ•°é‡: {len(small_keys)}")
        
        if not large_keys:
            self._logger.warning(f"ç­–ç•¥ {seed_strategy} æ²¡æœ‰ç”Ÿæˆä»»ä½•ç§å­")
            return []


        # 1. ç§å­æ½œåŠ›è¯„ä¼°å™¨
        class SeedPotentialAnalyzer:
            def __init__(self, hash_buckets, small_keys, box_capacity, min_items):
                self.hash_buckets = hash_buckets
                self.small_keys = sorted(small_keys)
                self.box_capacity = box_capacity
                self.min_items = min_items
                
            def calculate_potential(self, seed_key: int) -> float:
                """è®¡ç®—ç§å­çš„è£…ç®±æˆåŠŸæ½œåŠ›"""
                remaining_capacity = self.box_capacity - seed_key
                
                if remaining_capacity <= 0:
                    return 0.0
                
                # ç»Ÿè®¡å°å…ƒç´ çš„åˆ†å¸ƒ
                available_small_items = sum(
                    len(self.hash_buckets[k]) for k in self.small_keys 
                    if k in self.hash_buckets
                )
                
                if available_small_items == 0:
                    return 0.0
                
                # ä¼°ç®—èƒ½è£…å…¥çš„ç‰©å“æ•°é‡ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
                min_small_key = min(self.small_keys) if self.small_keys else remaining_capacity
                max_possible_items = remaining_capacity // min_small_key
                
                # è€ƒè™‘å®é™…å¯ç”¨æ€§ï¼ˆä¸æ˜¯æ‰€æœ‰å°keyéƒ½æœ‰å…ƒç´ ï¼‰
                practical_items = min(max_possible_items, available_small_items // 2)
                total_items = practical_items + 1  # +1 for seed
                
                # æ½œåŠ›è¯„åˆ†
                count_score = min(total_items / self.min_items, 1.0) if self.min_items > 0 else 1.0
                capacity_score = seed_key / self.box_capacity
                diversity_score = len([k for k in self.small_keys if k <= remaining_capacity]) / len(self.small_keys) if self.small_keys else 0
                
                return count_score * 0.5 + capacity_score * 0.3 + diversity_score * 0.2

        # 2. å¢å¼ºçš„å…±äº«èµ„æºç®¡ç†å™¨
        class EnhancedSharedManager:
            def __init__(self, hash_buckets, small_keys, large_keys):
                self.lock = threading.RLock()
                self.hash_buckets = hash_buckets
                self.small_keys = sorted(small_keys)
                self.large_keys = large_keys
                
                # ç»´æŠ¤å¯ç”¨keyçš„ç»Ÿè®¡ä¿¡æ¯
                self.key_stats = {}
                self._update_key_stats()
                
                # æ€§èƒ½ç»Ÿè®¡
                self.stats = {
                    'total_attempts': 0,
                    'successful_boxes': 0,
                    'failed_by_count': 0,
                    'failed_by_ratio': 0,
                    'failed_by_capacity': 0
                }
            
            def _update_key_stats(self):
                """æ›´æ–°keyçš„ç»Ÿè®¡ä¿¡æ¯"""
                self.key_stats = {}
                for k in self.small_keys:
                    if k in self.hash_buckets and len(self.hash_buckets[k]) > 0:
                        self.key_stats[k] = len(self.hash_buckets[k])
                        
            def get_seed_item(self, seed_key: int) -> Tuple[bool, Optional[np.record]]:
                """è·å–ç§å­å…ƒç´ """
                with self.lock:
                    if (seed_key in self.hash_buckets and 
                        len(self.hash_buckets[seed_key]) > 0):
                        item = self.hash_buckets[seed_key][0]
                        self.hash_buckets[seed_key] = self.hash_buckets[seed_key][1:]
                        return True, item
                    return False, None
            
            def get_item_by_key(self, target_key: int) -> Tuple[bool, Optional[np.record]]:
                """è·å–æŒ‡å®škeyçš„å…ƒç´ """
                with self.lock:
                    if (target_key in self.hash_buckets and 
                        len(self.hash_buckets[target_key]) > 0):
                        item = self.hash_buckets[target_key][0]
                        self.hash_buckets[target_key] = self.hash_buckets[target_key][1:]
                        
                        # æ›´æ–°ç»Ÿè®¡
                        if target_key in self.key_stats:
                            self.key_stats[target_key] -= 1
                            if self.key_stats[target_key] <= 0:
                                del self.key_stats[target_key]
                        
                        return True, item
                    return False, None

            def get_available_keys_with_counts(self) -> Dict[int, int]:
                """è·å–å¯ç”¨keyåŠå…¶å…ƒç´ æ•°é‡"""
                with self.lock:
                    return self.key_stats.copy()
            
            def rollback_items(self, items_to_rollback: List[Tuple[int, np.record]]):
                """å›æ»šå¤±è´¥è£…ç®±çš„å…ƒç´ """
                with self.lock:
                    for key, item in reversed(items_to_rollback):  # é€†åºå›æ»š
                        self.hash_buckets[key] = np.insert(self.hash_buckets[key], 0, item)
                        # æ›´æ–°ç»Ÿè®¡
                        if key in self.small_keys:
                            self.key_stats[key] = self.key_stats.get(key, 0) + 1
            
            def update_stats(self, result_type: str):
                """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
                with self.lock:
                    self.stats['total_attempts'] += 1
                    if result_type in self.stats:
                        self.stats[result_type] += 1
            
            def get_current_stats(self) -> Dict:
                """è·å–å½“å‰ç»Ÿè®¡"""
                with self.lock:
                    total_small_items = sum(
                        len(self.hash_buckets[k]) for k in self.small_keys 
                        if k in self.hash_buckets
                    )
                    return {
                        **self.stats,
                        'remaining_small_items': total_small_items,
                        'available_key_types': len(self.key_stats)
                    }

        # 3. æ™ºèƒ½è£…ç®±ç­–ç•¥
        def is_feasible_quick_check(remaining_capacity: int, current_items: int, 
                                   available_keys: Dict[int, int], min_items: int) -> bool:
            """å¿«é€Ÿå¯è¡Œæ€§æ£€æŸ¥"""
            if current_items >= min_items:
                return True
                
            needed_items = min_items - current_items
            if not available_keys:
                return False
            
            # è´ªå¿ƒä¼°ç®—ï¼šä¼˜å…ˆä½¿ç”¨å°key
            sorted_keys = sorted(available_keys.keys())
            possible_items = 0
            remaining_cap = remaining_capacity
            
            for key in sorted_keys:
                if remaining_cap <= 0:
                    break
                max_from_this_key = min(remaining_cap // key, available_keys[key])
                possible_items += max_from_this_key
                remaining_cap -= max_from_this_key * key
                
                if possible_items >= needed_items:
                    return True
                    
            return False
    
        def select_optimal_key(strategy: str, available_keys: Dict[int, int], 
                              remaining_capacity: int, current_items: int, min_items: int) -> Optional[int]:
            """æ ¹æ®ç­–ç•¥é€‰æ‹©æœ€ä¼˜key"""
            suitable_keys = [k for k in available_keys.keys() 
                            if k <= remaining_capacity and available_keys[k] > 0]
            if not suitable_keys:
                return None
            
            if strategy == "prioritize_count":
                # ä¼˜å…ˆæ•°é‡ï¼šé€‰æ‹©æœ€å°çš„key
                return min(suitable_keys)
            elif strategy == "prioritize_capacity":
                # ä¼˜å…ˆå®¹é‡ï¼šé€‰æ‹©æœ€å¤§çš„key
                return max(suitable_keys)
            elif strategy == "balanced":
                # å¹³è¡¡ç­–ç•¥ï¼šé€‰æ‹©ä¸­ç­‰å¤§å°ï¼Œä½†è€ƒè™‘å¯ç”¨æ•°é‡
                suitable_keys.sort()
                # å€¾å‘äºé€‰æ‹©æœ‰è¾ƒå¤šå¯ç”¨å…ƒç´ çš„key
                key_scores = [(k, available_keys[k] * (remaining_capacity / k)) for k in suitable_keys]
                key_scores.sort(key=lambda x: x[1], reverse=True)
                return key_scores[0][0]
            else:
                return suitable_keys[0]

        # 4. æ ¸å¿ƒè£…ç®±å‡½æ•°
        def pack_single_seed_with_constraints(seed_key: int, shared_manager: EnhancedSharedManager, 
                                            thread_id: int) -> Tuple:
            """å¸¦çº¦æŸçš„å•ç§å­è£…ç®±"""
            try:
                # è·å–ç§å­
                success, seed_item = shared_manager.get_seed_item(seed_key)
                if not success:
                    shared_manager.update_stats('failed_by_capacity')
                    return False, None, thread_id, 0, 0, "æ— å¯ç”¨ç§å­"
                
                current_box = [seed_item]
                used_items = [(seed_key, seed_item)]  # ç”¨äºå›æ»š
                remaining_capacity = box_capacity - seed_key
                items_count = 1
                
                max_iterations = min_items * 16  # é˜²æ­¢æ— é™å¾ªç¯  ç”± 5--->15(12 for 16384)
                iteration = 0
                
                # è£…ç®±ä¸»å¾ªç¯
                while (remaining_capacity > 0 and 
                       items_count < min_items * 8 and  # å…è®¸è¶…è¿‡æœ€å°æ•°é‡ï¼ˆå¯èƒ½æœ‰éå¸¸å°çš„å€¼ï¼‰(5 for 16384)
                       iteration < max_iterations):
                    
                    iteration += 1
                    available_keys = shared_manager.get_available_keys_with_counts()
                    
                    # å¿«é€Ÿå¯è¡Œæ€§æ£€æŸ¥
                    if (items_count < min_items and 
                        not is_feasible_quick_check(remaining_capacity, items_count, 
                                                  available_keys, min_items)):
                        # æ— æ³•è¾¾åˆ°æœ€å°ç‰©å“æ•°ï¼Œæå‰é€€å‡º
                        shared_manager.rollback_items(used_items)
                        shared_manager.update_stats('failed_by_count')
                        return False, None, thread_id, 0, items_count, f"æ— æ³•è¾¾åˆ°{min_items}ä¸ªç‰©å“"
                    
                    # åŠ¨æ€ç­–ç•¥é€‰æ‹©
                    if items_count < min_items * 0.8:
                        strategy = "prioritize_count"
                    elif items_count < min_items:
                        strategy = "balanced"
                    else:
                        strategy = "prioritize_capacity"
                    
                    # é€‰æ‹©ä¸‹ä¸€ä¸ªkey
                    target_key = select_optimal_key(strategy, available_keys, 
                                                  remaining_capacity, items_count, min_items)
                    if target_key is None:
                        break
                    
                    # è·å–å…ƒç´ 
                    success, item = shared_manager.get_item_by_key(target_key)
                    if not success:
                        continue  # è¯¥keyå·²è¢«å…¶ä»–çº¿ç¨‹ç”¨å®Œ
                    
                    current_box.append(item)
                    used_items.append((target_key, item))
                    remaining_capacity -= target_key
                    items_count += 1
                    
                    # å¦‚æœè¾¾åˆ°å®Œç¾è£…è½½ï¼Œå¯ä»¥æå‰ç»“æŸ
                    if remaining_capacity == 0 and items_count >= min_items:
                        break
                
                # æ£€æŸ¥æ‰€æœ‰çº¦æŸ
                current_capacity = box_capacity - remaining_capacity
                load_ratio = current_capacity / box_capacity
                
                meets_count = items_count >= min_items
                meets_ratio = load_ratio >= min_ratio
                meets_capacity = remaining_capacity >= 0
                
                success = meets_count and meets_ratio and meets_capacity
                
                if success:
                    shared_manager.update_stats('successful_boxes')
                    return True, current_box, thread_id, current_capacity, items_count, f"æˆåŠŸï¼š{items_count}ä¸ªç‰©å“ï¼Œè£…è½½ç‡{load_ratio:.1%}"
                else:
                    # è£…ç®±å¤±è´¥ï¼Œå›æ»š
                    shared_manager.rollback_items(used_items)
                    if not meets_count:
                        shared_manager.update_stats('failed_by_count')
                        reason = f"ç‰©å“æ•°ä¸è¶³ï¼š{items_count}<{min_items}"
                    elif not meets_ratio:
                        shared_manager.update_stats('failed_by_ratio')
                        reason = f"è£…è½½ç‡ä¸è¶³ï¼š{load_ratio:.1%}<{min_ratio:.1%}"
                    else:
                        shared_manager.update_stats('failed_by_capacity')
                        reason = "å®¹é‡çº¦æŸå¤±è´¥"
                    
                    return False, None, thread_id, current_capacity, items_count, reason
                    
            except Exception as e:
                shared_manager.update_stats('failed_by_capacity')
                return False, None, thread_id, 0, 0, f"è£…ç®±å¼‚å¸¸: {str(e)}"

        # 5. ç§å­é¢„å¤„ç†å’Œç­›é€‰
        analyzer = SeedPotentialAnalyzer(self.hash_buckets, small_keys, box_capacity, min_items)
        
        # æ”¶é›†å¹¶è¯„ä¼°æ‰€æœ‰ç§å­
        seed_candidates = []
        for key in large_keys:
            if key in self.hash_buckets:
                potential = analyzer.calculate_potential(key)
                count = len(self.hash_buckets[key])
                for _ in range(count):
                    seed_candidates.append((key, potential))  # ç¡®ä¿æ˜¯å…ƒç»„
        if not seed_candidates:
            self._logger.warning("æ²¡æœ‰å¯ç”¨çš„ç§å­å€™é€‰")
            return []
        
        # æŒ‰æ½œåŠ›æ’åºï¼Œåªå¤„ç†é«˜æ½œåŠ›ç§å­
        seed_candidates.sort(key=lambda x: x[1], reverse=True)
        potential_threshold = 0.2  # åªå¤„ç†æ½œåŠ›>0.2çš„ç§å­
        # high_potential_seeds = [seed for seed, potential in seed_candidates if potential > potential_threshold]
        # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†ç­›é€‰é€»è¾‘
        high_potential_candidates = [(seed, potential) for seed, potential in seed_candidates 
                                if potential > potential_threshold]

        # æœ€åä¿åº•ï¼šè‡³å°‘ä¿ç•™å‰50%çš„ç§å­
        if len(high_potential_candidates) < len(seed_candidates) * 0.5:
            mid_point = len(seed_candidates) // 2
            high_potential_candidates = seed_candidates[:mid_point]
        
        # ä¿®å¤ï¼šæ­£ç¡®æå–ç§å­åˆ—è¡¨
        selected_seeds = [seed for seed, potential in high_potential_candidates]
        
        self._logger.info(f"ç§å­ç­›é€‰å®Œæˆ:")
        self._logger.info(f"  ğŸ“Š æ€»ç§å­æ•°: {len(seed_candidates)}")
        self._logger.info(f"  ğŸ¯ ç­›é€‰å: {len(selected_seeds)}")
        self._logger.info(f"  ğŸš€ ç­›é€‰ç‡: {len(selected_seeds)/len(seed_candidates):.1%}")
        self._logger.info(f"  ğŸ”§ çº¿ç¨‹æ•°: {max_workers}")
        self._logger.info(f"  ğŸ“¦ çº¦æŸ: å®¹é‡â‰¥{min_ratio:.0%}, ç‰©å“â‰¥{min_items}ä¸ª")
    
        # 6. åˆå§‹åŒ–å…±äº«ç®¡ç†å™¨
        shared_manager = EnhancedSharedManager(self.hash_buckets, small_keys, large_keys)
        initial_stats = shared_manager.get_current_stats()
        
        self._logger.info(f"åˆå§‹èµ„æºçŠ¶æ€:")
        self._logger.info(f"  ğŸ—‚ï¸ å°å…ƒç´ æ€»æ•°: {initial_stats['remaining_small_items']}")
        self._logger.info(f"  ğŸ”‘ å¯ç”¨å°keyç§ç±»: {initial_stats['available_key_types']}")

        # 7. å¤šçº¿ç¨‹æ‰§è¡Œè£…ç®±
        output_boxes = []
        detailed_results = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=max_workers, 
                               thread_name_prefix="ConstraintPack") as executor:
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_seed = {}
            for i, seed_key in enumerate(selected_seeds):
                future = executor.submit(pack_single_seed_with_constraints, seed_key, shared_manager, i)
                future_to_seed[future] = (seed_key, i)
            
            # å¤„ç†ç»“æœ
            with tqdm(total=len(selected_seeds), unit='seed', 
                     desc='å¤šçº¦æŸè£…ç®±', dynamic_ncols=True) as pbar:
                
                completed_tasks = 0
                for future in as_completed(future_to_seed):
                    seed_key, task_id = future_to_seed[future]
                    
                    try:
                        success, box, thread_id, capacity, item_count, info = future.result(timeout=60)
                        
                        if success and box is not None:
                            output_boxes.append(np.array(box, dtype=self.DTYPE_SAMPLE_INFO))
                        
                        detailed_results.append({
                            'seed_key': seed_key,
                            'success': success,
                            'capacity': capacity,
                            'item_count': item_count,
                            'info': info,
                            'thread_id': thread_id
                        })
    
                        completed_tasks += 1
                        pbar.update(1)
                        
                        # æ¯100ä¸ªä»»åŠ¡æ›´æ–°ä¸€æ¬¡è¿›åº¦æè¿°
                        if completed_tasks % 100 == 0:
                            current_stats = shared_manager.get_current_stats()
                            success_rate = current_stats['successful_boxes'] / max(1, current_stats['total_attempts'])
                            pbar.set_description(
                                f'å¤šçº¦æŸè£…ç®±(æˆåŠŸ:{current_stats["successful_boxes"]}, '
                                f'æˆåŠŸç‡:{success_rate:.1%}, '
                                f'å‰©ä½™:{current_stats["remaining_small_items"]})'
                            )
                            
                    except Exception as e:
                        self._logger.error(f"ä»»åŠ¡ {task_id} (ç§å­={seed_key}) å¤±è´¥: {e}")
                        detailed_results.append({
                            'seed_key': seed_key,
                            'success': False,
                            'capacity': 0,
                            'item_count': 0,
                            'info': f"ä»»åŠ¡å¼‚å¸¸: {str(e)}",
                            'thread_id': -1
                        })
                        pbar.update(1)
    
        end_time = time.time()

        # 8. è¯¦ç»†ç»Ÿè®¡åˆ†æ
        final_stats = shared_manager.get_current_stats()
        
        # æŒ‰å¤±è´¥åŸå› åˆ†ç±»
        failure_analysis = defaultdict(int)
        success_details = []
        
        for result in detailed_results:
            if result['success']:
                success_details.append(result)
            else:
                # ç®€åŒ–å¤±è´¥åŸå› 
                info = result['info']
                if 'ç‰©å“æ•°ä¸è¶³' in info:
                    failure_analysis['ç‰©å“æ•°é‡ä¸è¶³'] += 1
                elif 'è£…è½½ç‡ä¸è¶³' in info:
                    failure_analysis['è£…è½½ç‡ä¸è¶³'] += 1
                elif 'æ— å¯ç”¨ç§å­' in info:
                    failure_analysis['ç§å­è€—å°½'] += 1
                elif 'æ— æ³•è¾¾åˆ°' in info:
                    failure_analysis['å¯è¡Œæ€§æ£€æŸ¥å¤±è´¥'] += 1
                else:
                    failure_analysis['å…¶ä»–åŸå› '] += 1

        # 9. è¾“å‡ºè¯¦ç»†æŠ¥å‘Š
        if output_boxes:
            # æˆåŠŸè£…ç®±çš„ç»Ÿè®¡
            total_items_packed = sum(len(box) for box in output_boxes)
            avg_items_per_box = total_items_packed / len(output_boxes)
            capacities = [result['capacity'] for result in success_details]
            avg_capacity = sum(capacities) / len(capacities) if capacities else 0
            avg_load_ratio = avg_capacity / box_capacity
            
            item_counts = [result['item_count'] for result in success_details]
            min_items_in_box = min(item_counts) if item_counts else 0
            max_items_in_box = max(item_counts) if item_counts else 0
            
            self._logger.info(f"ğŸ‰ å¤šçº¦æŸè£…ç®±å®Œæˆ!")
            self._logger.info(f"ğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
            self._logger.info(f"  â±ï¸  æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
            self._logger.info(f"  ğŸ¯ å¤„ç†ç§å­: {len(selected_seeds)}")
            self._logger.info(f"  ğŸ“¦ æˆåŠŸç®±å­: {len(output_boxes)}")
            self._logger.info(f"  ğŸ“ˆ æ€»æˆåŠŸç‡: {len(output_boxes)/len(selected_seeds):.2%}")
            
            self._logger.info(f"ğŸ“¦ è£…ç®±è´¨é‡:")
            self._logger.info(f"  ğŸ“Š å¹³å‡è£…è½½ç‡: {avg_load_ratio:.1%}")
            self._logger.info(f"  ğŸ”¢ å¹³å‡ç‰©å“æ•°: {avg_items_per_box:.1f}")
            self._logger.info(f"  ğŸ“‰ ç‰©å“æ•°èŒƒå›´: {min_items_in_box}-{max_items_in_box}")
            self._logger.info(f"  ğŸ’¾ æ€»æ‰“åŒ…ç‰©å“: {total_items_packed}")
            
            self._logger.info(f"ğŸ”— å‰©ä½™èµ„æº:")
            self._logger.info(f"  ğŸ—‚ï¸ å°å…ƒç´ : {final_stats['remaining_small_items']}")
            self._logger.info(f"  ğŸ”‘ å¯ç”¨keyç±»å‹: {final_stats['available_key_types']}")
            
            if failure_analysis:
                self._logger.info(f"âŒ å¤±è´¥åˆ†æ:")
                for reason, count in failure_analysis.items():
                    percentage = count / len(selected_seeds) * 100
                    self._logger.info(f"     {reason}: {count}æ¬¡ ({percentage:.1f}%)")
        else:
            self._logger.warning("âš ï¸  æ²¡æœ‰æˆåŠŸè£…ç®±ä»»ä½•ç‰©å“!")
            self._logger.info(f"å¤±è´¥åŸå› åˆ†å¸ƒ: {dict(failure_analysis)}")
            self._logger.info(f"å»ºè®®:")
            self._logger.info(f"  1. é™ä½ min_items (å½“å‰: {min_items})")
            self._logger.info(f"  2. é™ä½ min_ratio (å½“å‰: {min_ratio})")
            self._logger.info(f"  3. æ£€æŸ¥æ•°æ®åˆ†å¸ƒæ˜¯å¦åˆç†")


        
        # åªè¾“å‡º 1 ä¸ªç”¨äºçŠ¶æ€è·Ÿè¸ªï¼Œè¾“å‡º3ä¸ªç”¨äºå®é™…åº”ç”¨
        return output_boxes#, failure_analysis, final_stats

    def pack_simplest_strategy(
        self,
        keys: List[int],
        m: int,
        box_capacity: int = 16384,
        min_ratio: float = 0.95,
        max_workers: int = None,
    ) -> List[np.ndarray]:
        """
        æç®€è£…ç®±ç­–ç•¥ï¼š
        1. ä»æŒ‡å®š keys ä¸­éšæœºé€‰ m ä¸ªç§å­ï¼›
        2. å…¶ä½™æ‰€æœ‰å‰©ä½™å…ƒç´ ä½œä¸ºè£…å¡«æ± ï¼›
        3. å¤šçº¿ç¨‹è£…ç®±ï¼ˆæˆåŠŸåˆ ï¼Œå¤±è´¥å›æ»šï¼‰ï¼›
        4. å‰©ä½™å…ƒç´ å•çº¿ç¨‹å…œåº•ï¼Œæœ€åä¸€æ‰¹å¼ºåˆ¶è¾“å‡ºå¹¶æ¸…ç©ºã€‚
        """
        import random
        import threading
        from concurrent.futures import ThreadPoolExecutor, as_completed

        if max_workers is None:
            max_workers = min(os.cpu_count(), 8)

        # ---------- 1. æ„é€ ç§å­æ±  & è£…å¡«æ±  ----------
        seed_pool = []           # [(key, item), ...]
        fill_buckets = defaultdict(list)   # key -> [item, ...]

        # 1.1 æ”¶é›†ç§å­æ±  & æœªé€‰ä¸­çš„åŒ key å…ƒç´ 
        for k in keys:
            if k not in self.hash_buckets or len(self.hash_buckets[k]) == 0:
                continue
            arr = self.hash_buckets[k]
            # éšæœºé€‰ m ä¸ªï¼Œè‹¥ä¸è¶³åˆ™å…¨é€‰
            chosen = random.sample(list(arr), min(m, len(arr)))
            seed_pool.extend([(k, item) for item in chosen])
            # æœªé€‰ä¸­çš„è¿›å…¥è£…å¡«æ± 
            mask = np.ones(len(arr), dtype=bool)
            idxs = [i for i, it in enumerate(arr) if it in chosen]
            mask[idxs] = False
            fill_buckets[k].extend(arr[mask])

        # 1.2 é keys çš„æ‰€æœ‰å…ƒç´ å½’å…¥è£…å¡«æ± 
        for k in self.hash_buckets:
            if k not in keys:
                fill_buckets[k].extend(self.hash_buckets[k])

        if not seed_pool:
            self._logger.warning("ç§å­æ± ä¸ºç©ºï¼Œç›´æ¥è¾“å‡ºå‰©ä½™å…ƒç´ ä¸ºä¸€ç®±")
            # å¼ºåˆ¶è¾“å‡ºä¸€ç®±
            leftover = []
            for k, items in fill_buckets.items():
                leftover.extend(items)
            if leftover:
                box = np.array(leftover, dtype=self.DTYPE_SAMPLE_INFO)
                # æ¸…ç©º hash_buckets
                for k in list(self.hash_buckets.keys()):
                    del self.hash_buckets[k]
                return [box]
            return []

        # ---------- 2. çº¿ç¨‹å®‰å…¨çš„èµ„æºç®¡ç†å™¨ ----------
        class SimpleManager:
            def __init__(self, seed_items, fill_dict, dtype):
                self.lock = threading.RLock()
                # ç§å­é˜Ÿåˆ—
                self.seed_q = seed_items[:]          # æ‹·è´
                # è£…å¡«æ± 
                self.fill = defaultdict(deque)
                for k, lst in fill_dict.items():
                    self.fill[k] = deque(lst)
                # ç»Ÿè®¡
                self.boxes = []
                self.attempts = 0
                self.success = 0
                self.dtype = dtype

            def pop_seed(self):
                with self.lock:
                    if not self.seed_q:
                        return None
                    return self.seed_q.pop()

            def pop_fill(self, key):
                with self.lock:
                    if not self.fill[key]:
                        return None
                    return self.fill[key].popleft()

            def add_box(self, box):
                with self.lock:
                    self.boxes.append(np.array(box, dtype=self.dtype))
                    self.success += 1

            def rollback(self, rollback_items):
                with self.lock:
                    for key, item in reversed(rollback_items):
                        self.fill[key].appendleft(item)

            def remaining_elements(self):
                with self.lock:
                    return sum(len(q) for q in self.fill.values())

            def all_items(self):
                with self.lock:
                    items = []
                    for k, q in self.fill.items():
                        items.extend(q)
                    return items

        from collections import deque
        # mgr = SimpleManager(seed_pool, fill_buckets)
        mgr = SimpleManager(seed_pool, fill_buckets, self.DTYPE_SAMPLE_INFO)

        # ---------- 3. å¤šçº¿ç¨‹è£…ç®± ----------
        def pack_once(args):
            seed_key, seed_item, tid = args
            box = [seed_item]
            used = [(seed_key, seed_item)]
            rem = box_capacity - seed_key

            # è´ªå¿ƒè£…å¡«
            for k in sorted(mgr.fill.keys(), reverse=True):
                while rem >= k and mgr.fill[k]:
                    it = mgr.pop_fill(k)
                    if it is None:
                        break
                    box.append(it)
                    used.append((k, it))
                    rem -= k
                    if rem == 0:
                        break

            load = box_capacity - rem
            if load >= min_ratio * box_capacity:
                mgr.add_box(box)
                return True, tid, load
            else:
                mgr.rollback(used)
                return False, tid, load

        # æ„é€ ä»»åŠ¡åˆ—è¡¨
        tasks = [(k, it, i) for i, (k, it) in enumerate(mgr.seed_q)]
        mgr.seed_q.clear()   # æ¸…ç©ºï¼Œç”±ä»»åŠ¡åˆ—è¡¨å–ä»£

        with ThreadPoolExecutor(max_workers=max_workers) as exe:
            futs = [exe.submit(pack_once, t) for t in tasks]
            for f in as_completed(futs):
                ok, tid, load = f.result()
                mgr.attempts += 1

        # ---------- 4. å•çº¿ç¨‹å…œåº• ----------
        leftover_keys = list(mgr.fill.keys())
        random.shuffle(leftover_keys)

        while mgr.remaining_elements() > 0:
            # éšæœºæ‰¾ä¸€ä¸ªç§å­ï¼šä»å‰©ä½™ key ä¸­éšæœºæŒ‘ä¸€ä¸ªå…ƒç´ 
            candidates = [(k, mgr.fill[k][0]) for k in leftover_keys if mgr.fill[k]]
            if not candidates:
                break
            seed_key, seed_item = random.choice(candidates)
            mgr.pop_fill(seed_key)  # å–å‡ºä½œä¸ºç§å­

            box = [seed_item]
            used = [(seed_key, seed_item)]
            rem = box_capacity - seed_key

            # ç»§ç»­è£…å¡«
            for k in sorted(mgr.fill.keys(), reverse=True):
                while rem >= k and mgr.fill[k]:
                    it = mgr.pop_fill(k)
                    if it is None:
                        break
                    box.append(it)
                    used.append((k, it))
                    rem -= k
                    if rem == 0:
                        break

            # å¼ºåˆ¶è¾“å‡º
            mgr.add_box(box)

        # ---------- 5. åŒæ­¥å› self.hash_buckets ----------
        # æ­¤æ—¶ mgr.fill å·²å…¨éƒ¨æ¸…ç©ºï¼Œhash_buckets ç›´æ¥ç½®ç©º
        for k in list(self.hash_buckets.keys()):
            del self.hash_buckets[k]

        self._logger.info(
            f"pack_simplest_strategy å®Œæˆï¼šå¤šçº¿ç¨‹ä»»åŠ¡ {mgr.attempts}ï¼Œ"
            f"æˆåŠŸ {mgr.success}ï¼Œå…œåº•è¾“å‡º {len(mgr.boxes) - mgr.success} ç®±"
        )
        return mgr.boxes



    
    def check_hash_buckets_state(self):
        """æ£€æŸ¥å“ˆå¸Œæ¡¶çš„å½“å‰çŠ¶æ€"""
        total_items = sum(len(arr) for arr in self.hash_buckets.values())
        # total_keys = len(self.hash_buckets)
        total_keys = len([key for key in self.hash_buckets if len(self.hash_buckets[key])>0])  # æ²¡æœ‰åˆ é™¤ å…ƒç´ ä¸º0 çš„ key
        
        # æŒ‰keyå¤§å°åˆ†ç±»ç»Ÿè®¡
        key_distribution = defaultdict(int)
        for key in self.hash_buckets.keys():
            if key >= 8192:
                key_distribution['large'] += len(self.hash_buckets[key])
            elif key >= 2048:
                key_distribution['medium'] += len(self.hash_buckets[key])
            else:
                key_distribution['small'] += len(self.hash_buckets[key])
        
        print(f"å½“å‰å“ˆå¸Œæ¡¶çŠ¶æ€:")
        print(f"  ğŸ“¦ æ€»å…ƒç´ æ•°: {total_items}")
        print(f"  ğŸ”‘ æ€»keyæ•°: {total_keys}")
        print(f"  ğŸ“Š åˆ†å¸ƒæƒ…å†µ:")
        for size, count in key_distribution.items():
            print(f"    {size}: {count} ä¸ªå…ƒç´ ")
        
        return {
            'total_items': total_items,
            'total_keys': total_keys,
            'key_distribution': dict(key_distribution)
        }

# ###----------------------------------------------------------------------------------------------
# ###----------------------------------------------------------------------------------------------
# ###----------------------------------------------------------------------------------------------
# class PackingTracker:
#     """è£…ç®±æ“ä½œè¿½è¸ªå™¨"""
#     def __init__(self, processor):
#         self.processor = processor
#         self.history = []

#     def track_packing(self, strategy_name: str, **kwargs):
#         """è®°å½•ä¸€æ¬¡è£…ç®±æ“ä½œ"""

#         # è®°å½•æ“ä½œå‰çŠ¶æ€
#         before_state = self.processor.check_hash_buckets_state()

#         # æ‰§è¡Œè£…ç®±
#         boxes = getattr(self.processor, strategy_name)(**kwargs)

#         # è®°å½•æ“ä½œåçŠ¶æ€
#         after_state = self.processor.check_hash_buckets_state()

#         # è®¡ç®—å˜åŒ–
#         change = {
#             'strategy': strategy_name,
#             'kwargs': kwargs,
#             'before': before_state,
#             'after': after_state,
#             'boxes_count': len(boxes),
#             'items_used': before_state['total_items'] - after_state['total_items']
#         }

#         self.history.append(change)
#         return boxes

#     def print_summary(self):
#         """æ‰“å°è£…ç®±å†å²æ‘˜è¦"""
#         print("\n=== è£…ç®±æ“ä½œå†å² ===")
#         for i, op in enumerate(self.history, 1):
#             print(f"\næ“ä½œ {i}: {op['strategy']}")
#             print(f"å‚æ•°: {op['kwargs']}")
#             print(f"è£…ç®±æ•°: {op['boxes_count']}")
#             print(f"ä½¿ç”¨å…ƒç´ : {op['items_used']}")
#             print(f"æˆåŠŸç‡: {op['boxes_count'] / op['kwargs'].get('max_workers', 1):.1%}")

class PackingTracker:
    """è£…ç®±æ“ä½œè¿½è¸ªå™¨"""
    def __init__(self, processor):
        self.processor = processor
        self.history = []
        
    def track_packing(self, strategy_name: str, **kwargs):
        """è®°å½•ä¸€æ¬¡è£…ç®±æ“ä½œ"""
        before_state = self.processor.check_hash_buckets_state()
        # æ”¯æŒè¿”å›è¯¦ç»†ç»Ÿè®¡ï¼ˆå¦‚ total_attemptsï¼‰ï¼Œå¦åˆ™åªè¿”å›ç®±å­åˆ—è¡¨
        result = getattr(self.processor, strategy_name)(**kwargs)
        if isinstance(result, tuple) and len(result) >= 2 and isinstance(result[1], dict):
            boxes = result[0]
            stats = result[1]
            total_attempts = stats.get('total_attempts', len(boxes))
        else:
            boxes = result
            total_attempts = len(boxes)
        after_state = self.processor.check_hash_buckets_state()
        change = {
            'strategy': strategy_name,
            'kwargs': kwargs,
            'before': before_state,
            'after': after_state,
            'boxes_count': len(boxes),
            'items_used': before_state['total_items'] - after_state['total_items'],
            'total_attempts': total_attempts
        }
        self.history.append(change)
        return boxes
    
    def print_summary(self):
        """æ‰“å°è£…ç®±å†å²æ‘˜è¦"""
        print("\n=== è£…ç®±æ“ä½œå†å² ===")
        for i, op in enumerate(self.history, 1):
            print(f"\næ“ä½œ {i}: {op['strategy']}")
            print(f"å‚æ•°: {op['kwargs']}")
            print(f"è£…ç®±æ•°: {op['boxes_count']}")
            print(f"ä½¿ç”¨å…ƒç´ : {op['items_used']}")
            if op.get('total_attempts', 0):
                rate = op['boxes_count'] / op['total_attempts']
                print(f"æˆåŠŸç‡: {rate:.1%} ({op['boxes_count']}/{op['total_attempts']})")
            else:
                print(f"æˆåŠŸç‡: N/A")


# # ä½¿ç”¨ç¤ºä¾‹
# tracker = PackingTracker(processor)
# tracker.track_packing('pack_large_seed_parallel_multithread', 
#                      box_capacity=16384, min_ratio=0.95)
# tracker.track_packing('pack_with_min_items_constraint_multithread',
#                      box_capacity=16384, min_items=10, min_ratio=0.90)
# tracker.print_summary()


def analyze_packing_history(tracker):
    """åˆ†æè£…ç®±å†å²"""
    print("\n=== è¯¦ç»†åˆ†æ ===")
    
    total_boxes = sum(op['boxes_count'] for op in tracker.history)
    total_items = sum(op['items_used'] for op in tracker.history)
    
    print(f"æ€»è£…ç®±æ•°: {total_boxes}")
    print(f"æ€»ä½¿ç”¨å…ƒç´ : {total_items}")
    
    # åˆ†ææ¯ä¸ªç­–ç•¥çš„æ•ˆæœ
    strategy_stats = defaultdict(lambda: {'count': 0, 'items': 0, 'boxes': 0})
    for op in tracker.history:
        strategy = op['strategy']
        strategy_stats[strategy]['count'] += 1
        strategy_stats[strategy]['items'] += op['items_used']
        strategy_stats[strategy]['boxes'] += op['boxes_count']
    
    print("\nç­–ç•¥æ•ˆæœå¯¹æ¯”:")
    for strategy, stats in strategy_stats.items():
        avg_boxes = stats['boxes'] / stats['count']
        avg_items = stats['items'] / stats['count']
        print(f"{strategy}:")
        print(f"  å¹³å‡è£…ç®±æ•°: {avg_boxes:.1f}")
        print(f"  å¹³å‡ä½¿ç”¨å…ƒç´ : {avg_items:.0f}")
        print(f"  å¹³å‡æˆåŠŸç‡: {avg_boxes / 4:.1%}")  # å‡è®¾ä½¿ç”¨4ä¸ªçº¿ç¨‹
#### --------------------------------------- ####
## CKPT ç›¸å…³çš„ tools
import pickle

def save_ckpt(tracker, file_path: str):
    """
    ä¿å­˜ trackerï¼ˆåŒ…å« processorï¼‰åˆ°æ–‡ä»¶
    """
    with open(file_path, 'wb') as f:
        pickle.dump(tracker, f)
    print(f"å·²ä¿å­˜ckptåˆ° {file_path}")

def load_ckpt(file_path: str):
    """
    åŠ è½½ trackerï¼ˆåŒ…å« processorï¼‰çŠ¶æ€
    """
    with open(file_path, 'rb') as f:
        tracker = pickle.load(f)
    print(f"å·²åŠ è½½ckpt: {file_path}")
    return tracker

def save_bin_boxes(bin_boxes, file_path: str):
    """
    ä¿å­˜å•æ­¥è£…ç®±ç»“æœ
    """
    with open(file_path, 'wb') as f:
        pickle.dump(bin_boxes, f)
    print(f"å·²ä¿å­˜è£…ç®±ç»“æœåˆ° {file_path}")

def load_bin_boxes(file_path: str):
    """
    åŠ è½½å•æ­¥è£…ç®±ç»“æœ
    """
    with open(file_path, 'rb') as f:
        bin_boxes = pickle.load(f)
    print(f"å·²åŠ è½½è£…ç®±ç»“æœ: {file_path}")
    return bin_boxes
